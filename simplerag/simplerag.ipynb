{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion with text file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_docs = loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'title': 'Python (programming language) - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nPython (programming language) - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\n Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nHistory\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nDesign philosophy and features\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nSyntax and semantics\\n\\n\\n\\n\\nToggle Syntax and semantics subsection\\n\\n\\n\\n\\n\\n3.1\\nIndentation\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2\\nStatements and control flow\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3\\nExpressions\\n\\n\\n\\n\\n\\n\\n\\n\\n3.4\\nMethods\\n\\n\\n\\n\\n\\n\\n\\n\\n3.5\\nTyping\\n\\n\\n\\n\\n\\n\\n\\n\\n3.6\\nArithmetic operations\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nProgramming examples\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nLibraries\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nDevelopment environments\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nImplementations\\n\\n\\n\\n\\nToggle Implementations subsection\\n\\n\\n\\n\\n\\n7.1\\nReference implementation\\n\\n\\n\\n\\n\\n\\n\\n\\n7.2\\nOther implementations\\n\\n\\n\\n\\n\\n\\n\\n\\n7.3\\nNo longer supported implementations\\n\\n\\n\\n\\n\\n\\n\\n\\n7.4\\nCross-compilers to other languages\\n\\n\\n\\n\\n\\n\\n\\n\\n7.5\\nPerformance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8\\nDevelopment\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\nAPI documentation generators\\n\\n\\n\\n\\n\\n\\n\\n\\n10\\nNaming\\n\\n\\n\\n\\n\\n\\n\\n\\n11\\nPopularity\\n\\n\\n\\n\\n\\n\\n\\n\\n12\\nUses\\n\\n\\n\\n\\n\\n\\n\\n\\n13\\nLanguages influenced by Python\\n\\n\\n\\n\\n\\n\\n\\n\\n14\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n15\\nReferences\\n\\n\\n\\n\\nToggle References subsection\\n\\n\\n\\n\\n\\n15.1\\nSources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\nFurther reading\\n\\n\\n\\n\\n\\n\\n\\n\\n17\\nExternal links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nPython (programming language)\\n\\n\\n\\n111 languages\\n\\n\\n\\n\\nAfrikaansAlemannischالعربيةAragonésঅসমীয়াAsturianuAzərbaycancaتۆرکجهBasa Baliবাংলা閩南語 / Bân-lâm-gúБеларускаяБеларуская (тарашкевіца)भोजपुरीБългарскиBosanskiBrezhonegCatalàCebuanoČeštinaCymraegDanskDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalegoગુજરાતી한국어HausaՀայերենहिन्दीHrvatskiIdoBahasa IndonesiaInterlinguaÍslenskaItalianoעבריתქართულიҚазақшаKiswahiliKurdîКыргызчаLatinaLatviešuLietuviųLa .lojban.LombardMagyarМакедонскиമലയാളംमराठीBahasa MelayuМонголမြန်မာဘာသာNa Vosa VakavitiNederlandsनेपाली日本語ߒߞߏNorsk bokmålNorsk nynorskଓଡ଼ିଆOʻzbekcha / ўзбекчаਪੰਜਾਬੀپنجابیភាសាខ្មែរPlattdüütschPolskiPortuguêsQaraqalpaqshaRomânăRuna SimiРусскийСаха тылаᱥᱟᱱᱛᱟᱲᱤScotsShqipසිංහලSimple EnglishSlovenčinaSlovenščinaکوردیСрпски / srpskiSrpskohrvatski / српскохрватскиSuomiSvenskaTagalogதமிழ்Татарча / tatarçaၽႃႇသႃႇတႆး తెలుగుไทยТоҷикӣTürkçeBasa UgiУкраїнськаاردوئۇيغۇرچە / UyghurcheTiếng ViệtWalon文言Winaray吴语粵語中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikimedia CommonsMediaWikiWikibooksWikifunctionsWikiquoteWikiversity\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nGeneral-purpose programming language\\n\\n\\nPythonParadigmMulti-paradigm: object-oriented,[1] procedural (imperative), functional, structured, reflectiveDesigned\\xa0byGuido van RossumDeveloperPython Software FoundationFirst\\xa0appeared20\\xa0February 1991; 33 years ago\\xa0(1991-02-20)[2]Stable release3.13.0\\xa0\\n   / 7 October 2024; 12 days ago\\xa0(7 October 2024)\\nTyping disciplineduck, dynamic, strong;[3] optional type annotations (since 3.5, but those hints are ignored, except with unofficial tools)[4]OS\\nTier 1: 64-bit Linux, macOS; 64- and 32-bit Windows 10+[5]\\nTier 2: E.g. 32-bit WebAssembly (WASI)\\nTier 3: 64-bit Android,[6] iOS, FreeBSD, and (32-bit) Raspberry Pi OSUnofficial (or has been known to work): Other Unix-like/BSD variants) and a few other platforms[7][8][9]\\nLicensePython Software Foundation LicenseFilename extensions.py, .pyw, .pyz,[10]\\n.pyi, .pyc, .pydWebsitepython.orgMajor implementationsCPython, PyPy, Stackless Python, MicroPython, CircuitPython, IronPython, JythonDialectsCython, RPython, Starlark[11]Influenced byABC,[12] Ada,[13] ALGOL 68,[14] APL,[15] C,[16] C++,[17] CLU,[18] Dylan,[19] Haskell,[20][15] Icon,[21] Lisp,[22] Modula-3,[14][17] Perl,[23] Standard ML[15]InfluencedApache Groovy, Boo, Cobra, CoffeeScript,[24] D, F#, GDScript, Go, JavaScript,[25][26] Julia,[27] Mojo,[28] Nim, Ring,[29] Ruby,[30] Swift[31]\\n Python Programming at Wikibooks\\n\\nPython is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[32]\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.[33][34]\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python\\xa00.9.0.[35] Python\\xa02.0 was released in 2000. Python\\xa03.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python\\xa02.7.18, released in 2020, was the last release of Python\\xa02.[36]\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.[37][38][39][40]\\n\\n\\nHistory[edit]\\nMain article: History of Python\\nThe designer of Python, Guido van Rossum, at OSCON 2006\\nPython was invented in the late 1980s[41] by Guido van Rossum at Centrum Wiskunde & Informatica (CWI) in the Netherlands as a successor to the ABC programming language, which was inspired by SETL,[42] capable of exception handling and interfacing with the Amoeba operating system.[12] Its implementation began in December\\xa01989.[43] Van Rossum shouldered sole responsibility for the project, as the lead developer, until 12 July 2018, when he announced his \"permanent vacation\" from his responsibilities as Python\\'s \"benevolent dictator for life\" (BDFL), a title the Python community bestowed upon him to reflect his long-term commitment as the project\\'s chief decision-maker[44] (he has since come out of retirement and is self-titled \"BDFL-emeritus\"). In January\\xa02019, active Python core developers elected a five-member Steering Council to lead the project.[45][46]\\nPython 2.0 was released on 16 October 2000, with many major new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support.[47] Python\\xa03.0 was released on 3 December 2008, with many of its major features backported to Python\\xa02.6.x[48] and 2.7.x. Releases of Python\\xa03 include the 2to3 utility, which automates the translation of Python\\xa02 code to Python\\xa03.[49]\\nPython 2.7\\'s end-of-life was initially set for 2015, then postponed to 2020 out of concern that a large body of existing code could not easily be forward-ported to Python\\xa03.[50][51] No further security patches or other improvements will be released for it.[52][53] Currently only 3.9 and later are supported (2023 security issues were fixed in e.g. 3.7.17, the final 3.7.x release[54]). While Python 2.7 and older is officially unsupported, a different unofficial Python implementation, PyPy, continues to support Python 2, i.e. \"2.7.18+\" (plus 3.10), with the plus meaning (at least some) \"backported security updates\".[55]\\nIn 2021 (and again twice in 2022, and in September 2024 for Python 3.12.6 down to 3.8.20), security updates were expedited, since all Python versions were insecure (including 2.7[56]) because of security issues leading to possible remote code execution[57] and web-cache poisoning.[58] In 2022, Python\\xa03.10.4 and 3.9.12 were expedited[59] and 3.8.13, because of many security issues.[60] When Python\\xa03.9.13 was released in May 2022, it was announced that the 3.9 series (joining the older series 3.8 and 3.7) would only receive security fixes in the future.[61] On 7 September 2022, four new releases were made due to a potential denial-of-service attack: 3.10.7, 3.9.14, 3.8.14, and 3.7.14.[62][63]\\nEvery Python release since 3.5 has added some syntax to the language. 3.10 added the | union type operator[64] and the match and case keywords (for structural pattern matching statements). 3.11 expanded exception handling functionality. Python 3.12 added the new keyword type. Notable changes in 3.11 from 3.10 include increased program execution speed and improved error reporting.[65] Python 3.11 claims to be between 10 and 60% faster than Python 3.10, and Python 3.12 adds another 5% on top of that. It also has improved error messages, and many other changes.\\nPython 3.13 introduces more syntax for types, a new and improved interactive interpreter (REPL), featuring multi-line editing and color support; an incremental garbage collector (producing shorter pauses for collection in programs with a lot of objects, and addition to the improved speed in 3.11 and 3.12),  and an experimental just-in-time (JIT) compiler (such features, can/needs to be enabled specifically for the increase in speed),[66] and an experimental free-threaded build mode, which disables the global interpreter lock (GIL), allowing threads to run more concurrently, that latter feature enabled with python3.13t or python3.13t.exe.\\nPython 3.13 introduces some change in behavior, i.e. new \"well-defined semantics\", fixing bugs (plus many removals of deprecated classes, functions and methods, and removed some of the C\\xa0API and outdated modules): \"The  [old] implementation of locals() and frame.f_locals is slow, inconsistent and buggy [and it has] has many corner cases and oddities. Code that works around those may need to be changed. Code that uses locals() for simple templating, or print debugging, will continue to work correctly.\"[67]\\nSince 7\\xa0October\\xa02024[update], Python 3.13 is the latest stable release, and 3.13 and 3.12 are the only versions with active (as opposed to just security) support and Python 3.9 is the oldest supported version of Python (albeit in the \\'security support\\' phase), due to Python 3.8 reaching end-of-life.[68] Starting with 3.13, it and later versions have 2 years of full support (up from one and a half); followed by 3 years of security support (for same total support as before).\\nSome (more) standard library modules and many deprecated classes, functions and methods, will be removed in Python 3.15 or 3.16.[69][70]\\nPython 3.14 (now in alpha 1)[71] has changes for annotations, with PEP 649 \"[preserving] nearly all existing behavior of annotations from stock semantics\".[72]\\n\\nDesign philosophy and features[edit]\\nPython is a multi-paradigm programming language. Object-oriented programming and structured programming are fully supported, and many of their features support functional programming and aspect-oriented programming (including metaprogramming[73] and metaobjects).[74] Many other paradigms are supported via extensions, including design by contract[75][76] and logic programming.[77] Python is known as a glue language,[78] able to work very well with many other languages with ease of access.\\nPython uses dynamic typing and a combination of reference counting and a cycle-detecting garbage collector for memory management.[79] It uses dynamic name resolution (late binding), which binds method and variable names during program execution.\\nIts design offers some support for functional programming in the Lisp tradition. It has filter,mapandreduce functions; list comprehensions, dictionaries, sets, and generator expressions.[80] The standard library has two modules (itertools and functools) that implement functional tools borrowed from Haskell and Standard ML.[81]\\nIts core philosophy is summarized in the Zen of Python (PEP 20), which includes aphorisms such as:[82]\\n\\nBeautiful is better than ugly.\\nExplicit is better than implicit.\\nSimple is better than complex.\\nComplex is better than complicated.\\nReadability counts.\\nHowever, Python features regularly violate these principles and have received criticism for adding unnecessary language bloat.[83] Responses to these criticisms are that the Zen of Python is a guideline rather than a rule.[84] The addition of some new features had been so controversial that Guido van Rossum resigned as Benevolent Dictator for Life following vitriol over the addition of the assignment expression operator in Python 3.8.[85][86]\\nNevertheless, rather than building all of its functionality into its core, Python was designed to be highly extensible via modules. This compact modularity has made it particularly popular as a means of adding programmable interfaces to existing applications. Van Rossum\\'s vision of a small core language with a large standard library and easily extensible interpreter stemmed from his frustrations with ABC, which espoused the opposite approach.[41]\\nPython claims to strive for a simpler, less-cluttered syntax and grammar while giving developers a choice in their coding methodology. In contrast to Perl\\'s \"there is more than one way to do it\" motto, Python embraces a \"there should be one—and preferably only one—obvious way to do it.\" philosophy.[82] In practice, however, Python provides many ways to achieve the same task. There are, for example, at least three ways to format a string literal, with no certainty as to which one a programmer should use.[87] Alex Martelli, a Fellow at the Python Software Foundation and Python book author, wrote: \"To describe something as \\'clever\\' is not considered a compliment in the Python culture.\"[88]\\nPython\\'s developers usually strive to avoid premature optimization and reject patches to non-critical parts of the CPython reference implementation that would offer marginal increases in speed at the cost of clarity.[89] Execution speed can be improved by moving speed-critical functions to extension modules written in languages such as C, or by using a just-in-time compiler like PyPy. It is also possible to cross-compile to other languages, but it either doesn\\'t provide the full speed-up that might be expected, since Python is a very dynamic language, or a restricted subset of Python is compiled, and possibly semantics are slightly changed.[90]\\nPython\\'s developers aim for it to be fun to use. This is reflected in its name—a tribute to the British comedy group Monty Python[91]—and in occasionally playful approaches to tutorials and reference materials, such as the use of the terms \"spam\" and \"eggs\" (a reference to a Monty Python sketch) in examples, instead of the often-used \"foo\" and \"bar\".[92][93] A common neologism in the Python community is pythonic, which has a wide range of meanings related to program style. \"Pythonic\" code may use Python idioms well, be natural or show fluency in the language, or conform with Python\\'s minimalist philosophy and emphasis on readability. Code that is difficult to understand or reads like a rough transcription from another programming language is called unpythonic.[94]\\n\\nSyntax and semantics[edit]\\nMain article: Python syntax and semantics\\nAn example of Python code and indentation\\nExample of C# code with curly braces and semicolons\\nPython is meant to be an easily readable language. Its formatting is visually uncluttered and often uses English keywords where other languages use punctuation. Unlike many other languages, it does not use curly brackets to delimit blocks, and semicolons after statements are allowed but rarely used. It has fewer syntactic exceptions and special cases than C or Pascal.[95]\\n\\nIndentation[edit]\\nMain article: Python syntax and semantics §\\xa0Indentation\\nPython uses whitespace indentation, rather than curly brackets or keywords, to delimit blocks. An increase in indentation comes after certain statements; a decrease in indentation signifies the end of the current block.[96] Thus, the program\\'s visual structure accurately represents its semantic structure.[97] This feature is sometimes termed the off-side rule. Some other languages use indentation this way; but in most, indentation has no semantic meaning. The recommended indent size is four spaces.[98]\\n\\nStatements and control flow[edit]\\nPython\\'s statements include:\\n\\nThe assignment statement, using a single equals sign =\\nThe if statement, which conditionally executes a block of code, along with else and elif (a contraction of else-if)\\nThe for statement, which iterates over an iterable object, capturing each element to a local variable for use by the attached block\\nThe while statement, which executes a block of code as long as its condition is true\\nThe try statement, which allows exceptions raised in its attached code block to be caught and handled by except clauses (or new syntax except* in Python 3.11 for exception groups[99]); it also ensures that clean-up code in a finally block is always run regardless of how the block exits\\nThe raise statement, used to raise a specified exception or re-raise a caught exception\\nThe class statement, which executes a block of code and attaches its local namespace to a class, for use in object-oriented programming\\nThe def statement, which defines a function or method\\nThe with statement, which encloses a code block within a context manager (for example, acquiring a lock before it is run, then releasing the lock; or opening and closing a file), allowing resource-acquisition-is-initialization (RAII)-like behavior and replacing a common try/finally idiom[100]\\nThe break statement, which exits a loop\\nThe continue statement, which skips the rest of the current iteration and continues with the next\\nThe del statement, which removes a variable—deleting the reference from the name to the value, and producing an error if the variable is referred to before it is redefined\\nThe pass statement, serving as a NOP, syntactically needed to create an empty code block\\nThe assert statement, used in debugging to check for conditions that should apply\\nThe yield statement, which returns a value from a generator function (and also an operator); used to implement coroutines\\nThe return statement, used to return a value from a function\\nThe import and from statements, used to import modules whose functions or variables can be used in the current program\\nThe match and case statements, an analog of the switch statement construct, that compares an expression against one or more cases as a control-of-flow measure.\\nThe assignment statement (=) binds a name as a reference to a separate, dynamically allocated object. Variables may subsequently be rebound at any time to any object. In Python, a variable name is a generic reference holder without a fixed data type; however, it always refers to some object with a type. This is called dynamic typing—in contrast to statically-typed languages, where each variable may contain only a value of a certain type.\\nPython does not support tail call optimization or first-class continuations, and, according to Van Rossum, it never will.[101][102] However, better support for coroutine-like functionality is provided by extending Python\\'s generators.[103] Before 2.5, generators were lazy iterators; data was passed unidirectionally out of the generator. From Python\\xa02.5 on, it is possible to pass data back into a generator function; and from version 3.3, it can be passed through multiple stack levels.[104]\\n\\nExpressions[edit]\\nPython\\'s expressions include:\\n\\nThe +, -, and * operators for mathematical addition, subtraction, and multiplication are similar to other languages, but the behavior of division differs. There are two types of divisions in Python: floor division (or integer division) // and floating-point/division.[105] Python uses the ** operator for exponentiation.\\nPython uses the + operator for string concatenation. Python uses the * operator for duplicating a string a specified number of times.\\nThe @ infix operator. It is intended to be used by libraries such as NumPy for matrix multiplication.[106][107]\\nThe syntax :=, called the \"walrus operator\", was introduced in Python 3.8. It assigns values to variables as part of a larger expression.[108]\\nIn Python, == compares by value. Python\\'s is operator may be used to compare object identities (comparison by reference), and comparisons may be chained—for example, a <= b <= c.\\nPython uses and, or, and not as Boolean operators.\\nPython has a type of expression named a list comprehension, and a more general expression named a generator expression.[80]\\nAnonymous functions are implemented using lambda expressions; however, there may be only one expression in each body.\\nConditional expressions are written as x if c else y[109] (different in order of operands from the c\\xa0? x\\xa0: y operator common to many other languages).\\nPython makes a distinction between lists and tuples. Lists are written as [1, 2, 3], are mutable, and cannot be used as the keys of dictionaries (dictionary keys must be immutable in Python). Tuples, written as (1, 2, 3), are immutable and thus can be used as keys of dictionaries, provided all of the tuple\\'s elements are immutable. The + operator can be used to concatenate two tuples, which does not directly modify their contents, but produces a new tuple containing the elements of both. Thus, given the variable t initially equal to (1, 2, 3), executing t = t + (4, 5) first evaluates t + (4, 5), which yields (1, 2, 3, 4, 5), which is then assigned back to t—thereby effectively \"modifying the contents\" of t while conforming to the immutable nature of tuple objects. Parentheses are optional for tuples in unambiguous contexts.[110]\\nPython features sequence unpacking where multiple expressions, each evaluating to anything that can be assigned (to a variable, writable property, etc.) are associated in an identical manner to that forming tuple literals—and, as a whole, are put on the left-hand side of the equal sign in an assignment statement. The statement expects an iterable object on the right-hand side of the equal sign that produces the same number of values as the provided writable expressions; when iterated through them, it assigns each of the produced values to the corresponding expression on the left.[111]\\nPython has a \"string format\" operator % that functions analogously to printf format strings in C—e.g. \"spam=%s eggs=%d\" % (\"blah\", 2) evaluates to \"spam=blah eggs=2\". In Python\\xa02.6+ and 3+, this was supplemented by the format() method of the str class, e.g. \"spam={0} eggs={1}\".format(\"blah\", 2). Python\\xa03.6 added \"f-strings\": spam = \"blah\"; eggs = 2; f\\'spam={spam} eggs={eggs}\\'.[112]\\nStrings in Python can be concatenated by \"adding\" them (with the same operator as for adding integers and floats), e.g. \"spam\" + \"eggs\" returns \"spameggs\". If strings contain numbers, they are added as strings rather than integers, e.g. \"2\" + \"2\" returns \"22\".\\nPython has various string literals:\\nDelimited by single or double quotes; unlike in Unix shells, Perl, and Perl-influenced languages, single and double quotes work the same. Both use the backslash (\\\\) as an escape character. String interpolation became available in Python\\xa03.6 as \"formatted string literals\".[112]\\nTriple-quoted (beginning and ending with three single or double quotes), which may span multiple lines and function like here documents in shells, Perl, and Ruby.\\nRaw string varieties, denoted by prefixing the string literal with r. Escape sequences are not interpreted; hence raw strings are useful where literal backslashes are common, such as regular expressions and Windows-style paths. (Compare \"@-quoting\" in C#.)\\nPython has array index and array slicing expressions in lists, denoted as a[key], a[start:stop] or a[start:stop:step]. Indexes are zero-based, and negative indexes are relative to the end. Slices take elements from the start index up to, but not including, the stop index. The third slice parameter, called step or stride, allows elements to be skipped and reversed. Slice indexes may be omitted—for example, a[:] returns a copy of the entire list. Each element of a slice is a shallow copy.\\nIn Python, a distinction between expressions and statements is rigidly enforced, in contrast to languages such as Common Lisp, Scheme, or Ruby. This leads to duplicating some functionality. For example:\\n\\nList comprehensions vs. for-loops\\nConditional expressions vs. if blocks\\nThe eval() vs. exec() built-in functions (in Python\\xa02, exec is a statement); the former is for expressions, the latter is for statements\\nStatements cannot be a part of an expression—so list and other comprehensions or lambda expressions, all being expressions, cannot contain statements. A particular case is that an assignment statement such as a = 1 cannot form part of the conditional expression of a conditional statement.\\n\\nMethods[edit]\\nMethods on objects are functions attached to the object\\'s class; the syntax instance.method(argument) is, for normal methods and functions, syntactic sugar for Class.method(instance, argument). Python methods have an explicit self parameter to access instance data, in contrast to the implicit self (or this) in some other object-oriented programming languages (e.g., C++, Java, Objective-C, Ruby).[113] Python also provides methods, often called dunder methods (due to their names beginning and ending with double-underscores), to allow user-defined classes to modify how they are handled by native operations including length, comparison, in arithmetic operations and type conversion.[114]\\n\\nTyping[edit]\\nThe standard type hierarchy in Python\\xa03\\nPython uses duck typing and has typed objects but untyped variable names. Type constraints are not checked at compile time; rather, operations on an object may fail, signifying that it is not of a suitable type. Despite being dynamically typed, Python is strongly typed, forbidding operations that are not well-defined (for example, adding a number to a string) rather than silently attempting to make sense of them.\\nPython allows programmers to define their own types using classes, most often used for object-oriented programming. New instances of classes are constructed by calling the class (for example, SpamClass() or EggsClass()), and the classes are instances of the metaclass type (itself an instance of itself), allowing metaprogramming and reflection.\\nBefore version\\xa03.0, Python had two kinds of classes (both using the same syntax):  old-style and new-style;[115] current Python versions only support the semantics of the new style.\\nPython supports optional type annotations.[4][116] These annotations are not enforced by the language, but may be used by external tools such as mypy to catch errors.[117][118] Mypy also supports a Python compiler called mypyc, which leverages type annotations for optimization.[119]\\n\\n\\nSummary of Python 3\\'s built-in types\\n\\n\\nType\\n\\nMutability\\n\\nDescription\\n\\nSyntax examples\\n\\n\\nbool\\n\\nimmutable\\n\\nBoolean value\\n\\nTrueFalse\\n\\n\\nbytearray\\n\\nmutable\\n\\nSequence of bytes\\n\\nbytearray(b\\'Some ASCII\\')bytearray(b\"Some ASCII\")bytearray([119, 105, 107, 105])\\n\\n\\nbytes\\n\\nimmutable\\n\\nSequence of bytes\\n\\nb\\'Some ASCII\\'b\"Some ASCII\"bytes([119, 105, 107, 105])\\n\\n\\ncomplex\\n\\nimmutable\\n\\nComplex number with real and imaginary parts\\n\\n3+2.7j3 + 2.7j\\n\\n\\ndict\\n\\nmutable\\n\\nAssociative array (or dictionary) of key and value pairs; can contain mixed types (keys and values), keys must be a hashable type\\n\\n{\\'key1\\': 1.0, 3: False}{}\\n\\n\\ntypes.EllipsisType\\n\\nimmutable\\n\\nAn ellipsis placeholder to be used as an index in NumPy arrays\\n\\n...Ellipsis\\n\\n\\nfloat\\n\\nimmutable\\n\\nDouble-precision floating-point number. The precision is machine-dependent but in practice is generally implemented as a 64-bit IEEE\\xa0754 number with 53\\xa0bits of precision.[120]\\n\\n\\n1.33333\\n\\n\\n\\nfrozenset\\n\\nimmutable\\n\\nUnordered set, contains no duplicates; can contain mixed types, if hashable\\n\\nfrozenset([4.0, \\'string\\', True])\\n\\n\\nint\\n\\nimmutable\\n\\nInteger of unlimited magnitude[121]\\n\\n42\\n\\n\\nlist\\n\\nmutable\\n\\nList, can contain mixed types\\n\\n[4.0, \\'string\\', True][]\\n\\n\\ntypes.NoneType\\n\\nimmutable\\n\\nAn object representing the absence of a value, often called null in other languages\\n\\nNone\\n\\n\\ntypes.NotImplementedType\\n\\nimmutable\\n\\nA placeholder that can be returned from overloaded operators to indicate unsupported operand types.\\n\\nNotImplemented\\n\\n\\nrange\\n\\nimmutable\\n\\nAn immutable sequence of numbers commonly used for looping a specific number of times in for loops[122]\\n\\nrange(−1, 10)range(10, −5, −2)\\n\\n\\nset\\n\\nmutable\\n\\nUnordered set, contains no duplicates; can contain mixed types, if hashable\\n\\n{4.0, \\'string\\', True}set()\\n\\n\\nstr\\n\\nimmutable\\n\\nA character string: sequence of Unicode codepoints\\n\\n\\'Wikipedia\\'\"Wikipedia\"\"\"\"Spanning\\nmultiple\\nlines\"\"\"\\nSpanning\\nmultiple\\nlines\\n\\n\\n\\ntuple\\n\\nimmutable\\n\\nCan contain mixed types\\n\\n(4.0, \\'string\\', True)(\\'single element\\',)()\\n\\nArithmetic operations[edit]\\nPython has the usual symbols for arithmetic operators (+, -, *, /), the floor division operator // and the modulo operation % (where the remainder can be negative, e.g. 4\\xa0% -3 == -2). It also has ** for exponentiation, e.g. 5**3 == 125 and 9**0.5 == 3.0, and a matrix‑multiplication operator @ .[123] These operators work like in traditional math; with the same precedence rules, the operators infix (+ and - can also be unary to represent positive and negative numbers respectively).\\nThe division between integers produces floating-point results. The behavior of division has changed significantly over time:[124]\\n\\nCurrent Python (i.e. since 3.0) changed / to always be floating-point division, e.g. 5/2 == 2.5.\\nThe floor division // operator was introduced. So 7//3 == 2, -7//3 == -3, 7.5//3 == 2.0 and -7.5//3 == -3.0. Adding from __future__ import division causes a module used in Python 2.7 to use Python\\xa03.0 rules for division (see above).\\nIn Python terms, / is true division (or simply division), and // is floor division. / before version 3.0 is classic division.[124]\\nRounding towards negative infinity, though different from most languages, adds consistency. For instance, it means that the equation (a + b)//b == a//b + 1 is always true. It also means that the equation b*(a//b) + a%b == a is valid for both positive and negative values of a. However, maintaining the validity of this equation means that while the result of a%b is, as expected, in the half-open interval [0, b), where b is a positive integer, it has to lie in the interval (b, 0] when b is negative.[125]\\nPython provides a round function for rounding a float to the nearest integer. For tie-breaking, Python\\xa03 uses round to even: round(1.5) and round(2.5) both produce 2.[126] Versions before 3 used round-away-from-zero: round(0.5) is 1.0, round(-0.5) is −1.0.[127]\\nPython allows Boolean expressions with multiple equality relations in a manner that is consistent with general use in mathematics. For example, the expression a < b < c tests whether a is less than b and b is less than c.[128] C-derived languages interpret this expression differently: in C, the expression would first evaluate a < b, resulting in 0 or 1, and that result would then be compared with c.[129]\\nPython uses arbitrary-precision arithmetic for all integer operations. The Decimal type/class in the decimal module provides decimal floating-point numbers to a pre-defined arbitrary precision and several rounding modes.[130] The Fraction class in the fractions module provides arbitrary precision for rational numbers.[131]\\nDue to Python\\'s extensive mathematics library, and the third-party library NumPy that further extends the native capabilities, it is frequently used as a scientific scripting language to aid in problems such as numerical data processing and manipulation.[132][133]\\n\\nProgramming examples[edit]\\n\"Hello, World!\" program:\\n\\nprint(\\'Hello, world!\\')\\n\\nProgram to calculate the factorial of a positive integer:\\n\\nn = int(input(\\'Type a number, and its factorial will be printed: \\'))\\n\\nif n < 0:\\n    raise ValueError(\\'You must enter a non-negative integer\\')\\n\\nfactorial = 1\\nfor i in range(2, n + 1):\\n    factorial *= i\\n\\nprint(factorial)\\n\\nLibraries[edit]\\nPython\\'s large standard library[134] provides tools suited to many tasks and is commonly cited as one of its greatest strengths. For Internet-facing applications, many standard formats and protocols such as MIME and HTTP are supported. It includes modules for creating graphical user interfaces, connecting to relational databases, generating pseudorandom numbers, arithmetic with arbitrary-precision decimals,[130] manipulating regular expressions, and unit testing.\\nSome parts of the standard library are covered by specifications—for example, the Web Server Gateway Interface (WSGI) implementation wsgiref follows PEP 333[135]—but most are specified by their code, internal documentation, and test suites. However, because most of the standard library is cross-platform Python code, only a few modules need altering or rewriting for variant implementations.\\nAs of 17\\xa0March\\xa02024,[update] the Python Package Index (PyPI), the official repository for third-party Python software, contains over 523,000[136] packages with a wide range of functionality, including:\\n\\n\\nAutomation\\nData analytics\\nDatabases\\nDocumentation\\nGraphical user interfaces\\nImage processing\\nMachine learning\\nMobile apps\\nMultimedia\\nComputer networking\\nScientific computing\\nSystem administration\\nTest frameworks\\nText processing\\nWeb frameworks\\nWeb scraping\\nDevelopment environments[edit]\\nSee also: Comparison of integrated development environments §\\xa0Python\\nMost Python implementations (including CPython) include a read–eval–print loop (REPL), permitting them to function as a command line interpreter for which users enter statements sequentially and receive results immediately.\\nPython also comes with an Integrated development environment (IDE) called IDLE, which is more beginner-oriented.\\nOther shells, including IDLE and IPython, add further abilities such as improved auto-completion, session state retention, and syntax highlighting.\\nAs well as standard desktop integrated development environments including PyCharm, IntelliJ Idea, Visual Studio Code etc, there are web browser-based IDEs, including SageMath, for developing science- and math-related programs; PythonAnywhere, a browser-based IDE and hosting environment; and Canopy IDE, a commercial IDE emphasizing scientific computing.[137]\\n\\nImplementations[edit]\\nSee also: List of Python software §\\xa0Python implementations\\nReference implementation[edit]\\nCPython is the reference implementation of Python. It is written in C, meeting the C89 standard (Python 3.11 uses C11[138]) with several select C99 features. CPython includes its own C extensions, but third-party extensions are not limited to older C versions—e.g. they can be implemented with C11 or C++.[139][140] CPython compiles Python programs into an intermediate bytecode[141] which is then executed by its virtual machine.[142] CPython is distributed with a large standard library written in a mixture of C and native Python, and is available for many platforms, including Windows (starting with Python\\xa03.9, the Python installer deliberately fails to install on Windows 7 and 8;[143][144] Windows XP was supported until Python\\xa03.5) and most modern Unix-like systems, including macOS (and Apple M1 Macs, since Python\\xa03.9.1, with experimental installer), with unofficial support for VMS.[145] Platform portability was one of its earliest priorities.[146] (During Python\\xa01 and 2 development, even OS/2 and Solaris were supported,[147] but support has since been dropped for many platforms.)\\nAll current Python versions (i.e. since 3.7) only support operating systems with multi-threading support.\\n\\nOther implementations[edit]\\nAll alternative implementations have at least slightly different semantics (e.g. may have unordered dictionaries, unlike all current Python versions), e.g. with the larger Python ecosystem, such as with supporting the C Python API of with PyPy:\\n\\nPyPy is a fast, compliant interpreter of Python\\xa02.7 and  3.10.[148][149] Its just-in-time compiler often brings a significant speed improvement over CPython, but some libraries written in C cannot be used with it.[150] It has e.g. RISC-V support.\\nCodon is a language with an ahead-of-time (AOT) compiler, that (AOT) compiles a statically-typed Python-like language with \"syntax and semantics are nearly identical to Python\\'s, there are some notable differences\"[151] e.g. it uses 64-bit machine integers, for speed, not arbitrary like Python, and it claims speedups over CPython are usually on the order of 10–100x. It compiles to machine code (via LLVM) and supports native multithreading.[152]  Codon can also compile to Python extension modules that can be imported and used from Python.\\nStackless Python is a significant fork of CPython that implements microthreads; it does not use the call stack in the same way, thus allowing massively concurrent programs. PyPy also has a stackless version.[153]\\nMicroPython and CircuitPython are Python\\xa03 variants optimized for microcontrollers, including Lego Mindstorms EV3.[154]\\nPyston is a variant of the Python runtime that uses just-in-time compilation to speed up the execution of Python programs.[155]\\nCinder is a performance-oriented fork of CPython 3.8 that contains a number of optimizations, including bytecode inline caching, eager evaluation of coroutines, a method-at-a-time JIT, and an experimental bytecode compiler.[156]\\nSnek[157][158][159] Embedded Computing Language (compatible with e.g. 8-bit AVR microcontrollers such as ATmega 328P-based Arduino, as well as larger ones compatible with MicroPython) \"is Python-inspired, but it is not Python. It is possible to write Snek programs that run under a full Python system, but most Python programs will not run under Snek.\"[160] It is an imperative language not including OOP / classes, unlike Python, and simplifying to one number type with 32-bit single-precision (similar to JavaScript, except smaller).\\nNo longer supported implementations[edit]\\nOther just-in-time Python compilers have been developed, but are now unsupported:\\n\\nGoogle began a project named Unladen Swallow in 2009, with the aim of speeding up the Python interpreter five-fold by using the LLVM, and of improving its multithreading ability to scale to thousands of cores,[161] while ordinary implementations suffer from the global interpreter lock.\\nPsyco is a discontinued just-in-time specializing compiler that integrates with CPython and transforms bytecode to machine code at runtime. The emitted code is specialized for certain data types and is faster than the standard Python code. Psyco does not support Python\\xa02.7 or later.\\nPyS60 was a Python\\xa02 interpreter for Series 60 mobile phones released by Nokia in 2005. It implemented many of the modules from the standard library and some additional modules for integrating with the Symbian operating system. The Nokia N900 also supports Python with GTK widget libraries, enabling programs to be written and run on the target device.[162]\\nCross-compilers to other languages[edit]\\nThere are several compilers/transpilers to high-level object languages, with either unrestricted Python, a restricted subset of Python, or a language similar to Python as the source language:\\n\\nBrython,[163] Transcrypt[164][165] and Pyjs (latest release in 2012) compile Python to JavaScript.\\nCython compiles (a superset of) Python to C. The resulting code is also usable with Python via direct C-level API calls into the Python interpreter.\\nPyJL compiles/transpiles a subset of Python to \"human-readable, maintainable, and high-performance Julia source code\".[90] Despite claiming high performance, no tool can claim to do that for arbitrary Python code; i.e. it\\'s known not possible to compile to a faster language or machine code. Unless semantics of Python are changed, but in many cases speedup is possible with few or no changes in the Python code. The faster Julia source code can then be used from Python, or compiled to machine code, and based that way.\\nNuitka compiles Python into C.[166] It works with Python 3.4 to 3.12 (and 2.6 and 2.7), for Python\\'s main supported platforms (and Windows 7 or even Windows XP) and for Android. It claims complete support for Python 3.10,  some support for 3.11 and 3.12  and experimental support for Python 3.13. It supports macOS including Apple Silicon-based.  It\\'s a free compiler, though it also has commercial add-ons (e.g. for hiding source code).\\nNumba is used from Python, as a tool (enabled by adding a decorator to relevant Python code), a JIT compiler that translates a subset of Python and NumPy code into fast machine code.\\nPythran compiles a subset of Python\\xa03 to C++ (C++11).[167]\\nRPython can be compiled to C, and is used to build the PyPy interpreter of Python.\\nThe Python → 11l → C++ transpiler[168] compiles a subset of Python\\xa03 to C++ (C++17).\\nSpecialized:\\n\\nMyHDL is a Python-based hardware description language (HDL), that converts MyHDL code to Verilog or VHDL code.\\nOlder projects (or not to be used with Python 3.x and latest syntax):\\n\\nGoogle\\'s Grumpy (latest release in 2017) transpiles Python\\xa02 to Go.[169][170][171]\\nIronPython allows running Python\\xa02.7 programs (and an alpha, released in 2021, is also available for \"Python\\xa03.4, although features and behaviors from later versions may be included\"[172]) on the .NET Common Language Runtime.[173]\\nJython compiles Python\\xa02.7 to Java bytecode, allowing the use of the Java libraries from a Python program.[174]\\nPyrex (latest release in 2010) and Shed Skin (latest release in 2013) compile to C and C++ respectively.\\nPerformance[edit]\\nPerformance comparison of various Python implementations on a non-numerical (combinatorial) workload was presented at EuroSciPy \\'13.[175] Python\\'s performance compared to other programming languages is also benchmarked by The Computer Language Benchmarks Game.[176]\\n\\nDevelopment[edit]\\nPython\\'s development is conducted largely through the Python Enhancement Proposal (PEP) process, the primary mechanism for proposing major new features, collecting community input on issues, and documenting Python design decisions.[177] Python coding style is covered in PEP\\xa08.[178] Outstanding PEPs are reviewed and commented on by the Python community and the steering council.[177]\\nEnhancement of the language corresponds with the development of the CPython reference implementation. The mailing list python-dev is the primary forum for the language\\'s development. Specific issues were originally discussed in the Roundup bug tracker hosted at by the foundation.[179] In 2022, all issues and discussions were migrated to GitHub.[180] Development originally took place on a self-hosted source-code repository running Mercurial, until Python moved to GitHub in January 2017.[181]\\nCPython\\'s public releases come in three types, distinguished by which part of the version number is incremented:\\n\\nBackward-incompatible versions, where code is expected to break and needs to be manually ported. The first part of the version number is incremented. These releases happen infrequently—version 3.0 was released 8 years after 2.0. According to Guido van Rossum, a version 4.0 is very unlikely to ever happen.[182]\\nMajor or \"feature\" releases are largely compatible with the previous version but introduce new features. The second part of the version number is incremented. Starting with Python\\xa03.9, these releases are expected to happen annually.[183][184] Each major version is supported by bug fixes for several years after its release.[185]\\nBugfix releases,[186] which introduce no new features, occur about every 3 months and are made when a sufficient number of bugs have been fixed upstream since the last release. Security vulnerabilities are also patched in these releases. The third and final part of the version number is incremented.[186]\\nMany alpha, beta, and release-candidates are also released as previews and for testing before final releases. Although there is a rough schedule for each release, they are often delayed if the code is not ready. Python\\'s development team monitors the state of the code by running the large unit test suite during development.[187]\\nThe major academic conference on Python is PyCon. There are also special Python mentoring programs, such as PyLadies.\\nPython 3.12 removed wstr meaning Python extensions[188] need to be modified,[189] and 3.10 added pattern matching to the language.[190]\\nPython 3.12 dropped some outdated modules, and more will be dropped in the future, deprecated as of 3.13; already deprecated array \\'u\\' format code will emit DeprecationWarning since 3.13 and will be removed in Python 3.16. The \\'w\\' format code should be used instead. Part of ctypes is also deprecated and http.server.CGIHTTPRequestHandler will emit a DeprecationWarning, and will be removed in 3.15. Using that code already has a high potential for both security and functionality bugs. Parts of the typing module are deprecated, e.g. creating a typing.NamedTuple class using keyword arguments to denote the fields and such (and more) will be disallowed in Python 3.15.\\n\\nAPI documentation generators[edit]\\nTools that can generate documentation for Python API include pydoc (available as part of the standard library), Sphinx, Pdoc and its forks, Doxygen and Graphviz, among others.[191]\\n\\nNaming[edit]\\nPython\\'s name is derived from the British comedy group Monty Python, whom Python creator Guido van Rossum enjoyed while developing the language. Monty Python references appear frequently in Python code and culture;[192] for example, the metasyntactic variables often used in Python literature are spam and eggs instead of the traditional foo and bar.[192][193] The official Python documentation also contains various references to Monty Python routines.[194][195] Users of Python are sometimes referred to as \"Pythonistas\".[196]\\nThe prefix Py- is used to show that something is related to Python. Examples of the use of this prefix in names of Python applications or libraries include Pygame, a binding of Simple DirectMedia Layer to Python (commonly used to create games); PyQt and PyGTK, which bind Qt and GTK to Python respectively; and PyPy, a Python implementation originally written in Python.\\n\\nPopularity[edit]\\nSince 2003, Python has consistently ranked in the top ten most popular programming languages in the TIOBE Programming Community Index where as of December\\xa02022[update] it was the most popular language (ahead of C, C++, and Java).[39] It was selected as Programming Language of the Year (for \"the highest rise in ratings in a year\") in 2007, 2010, 2018, and 2020 (the only language to have done so four times as of 2020[update][197]).\\nLarge organizations that use Python include Wikipedia, Google,[198] Yahoo!,[199] CERN,[200] NASA,[201] Facebook,[202] Amazon, Instagram,[203] Spotify,[204] and some smaller entities like Industrial Light & Magic[205] and ITA.[206] The social news networking site Reddit was written mostly in Python.[207] Organizations that partially use Python include Discord[208] and Baidu.[209]\\n\\nUses[edit]\\nMain article: List of Python software\\nPython Powered\\nPython can serve as a scripting language for web applications, e.g. via mod_wsgi for the Apache webserver.[210] With Web Server Gateway Interface, a standard API has evolved to facilitate these applications. Web frameworks like Django, Pylons, Pyramid, TurboGears, web2py, Tornado, Flask, Bottle, and Zope support developers in the design and maintenance of complex applications. Pyjs and IronPython can be used to develop the client-side of Ajax-based applications. SQLAlchemy can be used as a data mapper to a relational database. Twisted is a framework to program communications between computers, and is used (for example) by Dropbox.\\nLibraries such as NumPy, SciPy and Matplotlib allow the effective use of Python in scientific computing,[211][212] with specialized libraries such as Biopython and Astropy providing domain-specific functionality. SageMath is a computer algebra system with a notebook interface programmable in Python: its library covers many aspects of mathematics, including algebra, combinatorics, numerical mathematics, number theory, and calculus.[213] OpenCV has Python bindings with a rich set of features for computer vision and image processing.[214]\\nPython is commonly used in artificial intelligence projects and machine learning projects with the help of libraries like TensorFlow, Keras, Pytorch, scikit-learn and the Logic language ProbLog.[215][216][217][218][219] As a scripting language with a modular architecture, simple syntax, and rich text processing tools, Python is often used for natural language processing.[220]\\nThe combination of Python and Prolog has proved to be particularly useful for AI applications, with Prolog providing knowledge representation and reasoning capabilities. The Janus system, in particular, exploits the similarities between these two languages,\\nin part because of their use of dynamic typing, and the simple recursive nature of their\\ndata structures. Typical applications of this combination include  natural language processing, visual query\\nanswering, geospatial reasoning, and handling of semantic web data.[221][222]\\nThe Natlog system, implemented in Python, uses Definite Clause Grammars (DCGs) as prompt generators for text-to-text generators like GPT3 and text-to-image generators like DALL-E or Stable Diffusion.[223]\\nPython can also be used for graphical user interface (GUI) by using libraries like Tkinter.[224][225]\\nPython has been successfully embedded in many software products as a scripting language, including in finite element method software such as Abaqus, 3D parametric modelers like FreeCAD, 3D animation packages such as 3ds Max, Blender, Cinema 4D, Lightwave, Houdini, Maya, modo, MotionBuilder, Softimage, the visual effects compositor Nuke, 2D imaging programs like GIMP,[226] Inkscape, Scribus and Paint Shop Pro,[227] and musical notation programs like scorewriter and capella. GNU Debugger uses Python as a pretty printer to show complex structures such as C++ containers. Esri promotes Python as the best choice for writing scripts in ArcGIS.[228] It has also been used in several video games,[229][230] and has been adopted as first of the three available programming languages in Google App Engine, the other two being Java and Go.[231]\\nMany operating systems include Python as a standard component. It ships with most Linux distributions,[232] AmigaOS 4 (using Python\\xa02.7), FreeBSD (as a package), NetBSD, and OpenBSD (as a package) and can be used from the command line (terminal). Many Linux distributions use installers written in Python: Ubuntu uses the Ubiquity installer, while Red Hat Linux and Fedora Linux use the Anaconda installer. Gentoo Linux uses Python in its package management system, Portage.\\nPython is used extensively in the information security industry, including in exploit development.[233][234]\\nMost of the Sugar software for the One Laptop per Child XO, developed at Sugar Labs as of 2008[update], is written in Python.[235] The Raspberry Pi single-board computer project has adopted Python as its main user-programming language.\\nLibreOffice includes Python and intends to replace Java with Python. Its Python Scripting Provider is a core feature[236] since Version 4.0 from 7 February 2013.\\n\\nLanguages influenced by Python[edit]\\nPython\\'s design and philosophy have influenced many other programming languages:\\n\\nBoo uses indentation, a similar syntax, and a similar object model.[237]\\nCobra uses indentation and a similar syntax, and its Acknowledgements document lists Python first among languages that influenced it.[238]\\nCoffeeScript, a programming language that cross-compiles to JavaScript, has Python-inspired syntax.\\nECMAScript–JavaScript borrowed iterators and generators from Python.[239]\\nGDScript, a scripting language very similar to Python, built-in to the Godot game engine.[240]\\nGo is designed for the \"speed of working in a dynamic language like Python\"[241] and shares the same syntax for slicing arrays.\\nGroovy was motivated by the desire to bring the Python design philosophy to Java.[242]\\nJulia was designed to be \"as usable for general programming as Python\".[27]\\nMojo is a non-strict[28][243] superset of Python (e.g. still missing classes, and adding e.g. struct).[244]\\nNim uses indentation and similar syntax.[245]\\nRuby\\'s creator, Yukihiro Matsumoto, has said: \"I wanted a scripting language that was more powerful than Perl, and more object-oriented than Python. That\\'s why I decided to design my own language.\"[246]\\nSwift, a programming language developed by Apple, has some Python-inspired syntax.[247]\\nKotlin blends Python and Java features, minimizing boilerplate code for enhanced developer efficiency.[248]\\nPython\\'s development practices have also been emulated by other languages. For example, the practice of requiring a document describing the rationale for, and issues surrounding, a change to the language (in Python, a PEP) is also used in Tcl,[249] Erlang,[250] and Swift.[251]\\n\\nSee also[edit]\\n\\nComputer programming portalFree and open-source software portal\\nPython syntax and semantics\\npip (package manager)\\nList of programming languages\\nHistory of programming languages\\nComparison of programming languages\\n\\nReferences[edit]\\n\\n\\n^ \"General Python FAQ – Python 3 documentation\". docs.python.org. Retrieved 7 July 2024.\\n\\n^ \"Python 0.9.1 part 01/21\". alt.sources archives. Archived from the original on 11 August 2021. Retrieved 11 August 2021.\\n\\n^ \"Why is Python a dynamic language and also a strongly typed language\". Python Wiki. Archived from the original on 14 March 2021. Retrieved 27 January 2021.\\n\\n^ a b \"PEP 483 – The Theory of Type Hints\". Python.org. Archived from the original on 14 June 2020. Retrieved 14 June 2018.\\n\\n^ \"PEP 11 – CPython platform support | peps.python.org\". Python Enhancement Proposals (PEPs). Retrieved 22 April 2024.\\n\\n^ \"PEP 738 – Adding Android as a supported platform | peps.python.org\". Python Enhancement Proposals (PEPs). Retrieved 19 May 2024.\\n\\n^ \"Download Python for Other Platforms\". Python.org. Archived from the original on 27 November 2020. Retrieved 18 August 2023.\\n\\n^ \"test – Regression tests package for Python – Python 3.7.13 documentation\". docs.python.org. Archived from the original on 17 May 2022. Retrieved 17 May 2022.\\n\\n^ \"platform – Access to underlying platform\\'s identifying data – Python 3.10.4 documentation\". docs.python.org. Archived from the original on 17 May 2022. Retrieved 17 May 2022.\\n\\n^ Holth, Moore (30 March 2014). \"PEP 0441 – Improving Python ZIP Application Support\". Archived from the original on 26 December 2018. Retrieved 12 November 2015.\\n\\n^ \"Starlark Language\". Archived from the original on 15 June 2020. Retrieved 25 May 2019.\\n\\n^ a b \"Why was Python created in the first place?\". General Python FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 22 March 2007. I had extensive experience with implementing an interpreted language in the ABC group at CWI, and from working with this group I had learned a lot about language design. This is the origin of many Python features, including the use of indentation for statement grouping and the inclusion of very high-level data types (although the details are all different in Python).\\n\\n^ \"Ada 83 Reference Manual (raise statement)\". Archived from the original on 22 October 2019. Retrieved 7 January 2020.\\n\\n^ a b Kuchling, Andrew M. (22 December 2006). \"Interview with Guido van Rossum (July 1998)\". amk.ca. Archived from the original on 1 May 2007. Retrieved 12 March 2012. I\\'d spent a summer at DEC\\'s Systems Research Center, which introduced me to Modula-2+; the Modula-3 final report was being written there at about the same time. What I learned there later showed up in Python\\'s exception handling, modules, and the fact that methods explicitly contain \\'self\\' in their parameter list. String slicing came from Algol-68 and Icon.\\n\\n^ a b c \"itertools – Functions creating iterators for efficient looping – Python 3.7.1 documentation\". docs.python.org. Archived from the original on 14 June 2020. Retrieved 22 November 2016. This module implements a number of iterator building blocks inspired by constructs from APL, Haskell, and SML.\\n\\n^ van Rossum, Guido (1993). \"An Introduction to Python for UNIX/C Programmers\". Proceedings of the NLUUG Najaarsconferentie (Dutch UNIX Users Group). CiteSeerX\\xa010.1.1.38.2023. even though the design of C is far from ideal, its influence on Python is considerable.\\n\\n^ a b \"Classes\". The Python Tutorial. Python Software Foundation. Archived from the original on 23 October 2012. Retrieved 20 February 2012. It is a mixture of the class mechanisms found in C++ and Modula-3\\n\\n^ Lundh, Fredrik. \"Call By Object\". effbot.org. Archived from the original on 23 November 2019. Retrieved 21 November 2017. replace \"CLU\" with \"Python\", \"record\" with \"instance\", and \"procedure\" with \"function or method\", and you get a pretty accurate description of Python\\'s object model.\\n\\n^ Simionato, Michele. \"The Python 2.3 Method Resolution Order\". Python Software Foundation. Archived from the original on 20 August 2020. Retrieved 29 July 2014. The C3 method itself has nothing to do with Python, since it was invented by people working on Dylan and it is described in a paper intended for lispers\\n\\n^ Kuchling, A. M. \"Functional Programming HOWTO\". Python v2.7.2 documentation. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 9 February 2012. List comprehensions and generator expressions [...] are a concise notation for such operations, borrowed from the functional programming language Haskell.\\n\\n^ Schemenauer, Neil; Peters, Tim; Hetland, Magnus Lie (18 May 2001). \"PEP 255\\xa0– Simple Generators\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 9 February 2012.\\n\\n^ \"More Control Flow Tools\". Python 3 documentation. Python Software Foundation. Archived from the original on 4 June 2016. Retrieved 24 July 2015. By popular demand, a few features commonly found in functional programming languages like Lisp have been added to Python. With the lambda keyword, small anonymous functions can be created.\\n\\n^ \"re – Regular expression operations – Python 3.10.6 documentation\". docs.python.org. Archived from the original on 18 July 2018. Retrieved 6 September 2022. This module provides regular expression matching operations similar to those found in Perl.\\n\\n^ \"CoffeeScript\". coffeescript.org. Archived from the original on 12 June 2020. Retrieved 3 July 2018.\\n\\n^ \"Perl and Python influences in JavaScript\". www.2ality.com. 24 February 2013. Archived from the original on 26 December 2018. Retrieved 15 May 2015.\\n\\n^ Rauschmayer, Axel. \"Chapter 3: The Nature of JavaScript; Influences\". O\\'Reilly, Speaking JavaScript. Archived from the original on 26 December 2018. Retrieved 15 May 2015.\\n\\n^ a b \"Why We Created Julia\". Julia website. February 2012. Archived from the original on 2 May 2020. Retrieved 5 June 2014. We want something as usable for general programming as Python [...]\\n\\n^ a b Krill, Paul (4 May 2023). \"Mojo language marries Python and MLIR for AI development\". InfoWorld. Archived from the original on 5 May 2023. Retrieved 5 May 2023.\\n\\n^ Ring Team (4 December 2017). \"Ring and other languages\". ring-lang.net. ring-lang. Archived from the original on 25 December 2018. Retrieved 4 December 2017.\\n\\n^ Bini, Ola (2007). Practical JRuby on Rails Web 2.0 Projects: bringing Ruby on Rails to the Java platform. Berkeley: APress. p.\\xa03. ISBN\\xa0978-1-59059-881-8.\\n\\n^ Lattner, Chris (3 June 2014). \"Chris Lattner\\'s Homepage\". Chris Lattner. Archived from the original on 25 December 2018. Retrieved 3 June 2014. The Swift language is the product of tireless effort from a team of language experts, documentation gurus, compiler optimization ninjas, and an incredibly important internal dogfooding group who provided feedback to help refine and battle-test ideas. Of course, it also greatly benefited from the experiences hard-won by many other languages in the field, drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.\\n\\n^ Kuhlman, Dave. \"A Python Book: Beginning Python, Advanced Python, and Python Exercises\". Section 1.1. Archived from the original (PDF) on 23 June 2012.\\n\\n^ \"About Python\". Python Software Foundation. Archived from the original on 20 April 2012. Retrieved 24 April 2012., second section \"Fans of Python use the phrase \"batteries included\" to describe the standard library, which covers everything from asynchronous processing to zip files.\"\\n\\n^ \"PEP 206 – Python Advanced Library\". Python.org. Archived from the original on 5 May 2021. Retrieved 11 October 2021.\\n\\n^ Rossum, Guido Van (20 January 2009). \"The History of Python: A Brief Timeline of Python\". The History of Python. Archived from the original on 5 June 2020. Retrieved 5 March 2021.\\n\\n^ Peterson, Benjamin (20 April 2020). \"Python 2.7.18, the last release of Python 2\". Python Insider. Archived from the original on 26 April 2020. Retrieved 27 April 2020.\\n\\n^ \"Stack Overflow Developer Survey 2022\". Stack Overflow. Archived from the original on 27 June 2022. Retrieved 12 August 2022.\\n\\n^ \"The State of Developer Ecosystem in 2020 Infographic\". JetBrains: Developer Tools for Professionals and Teams. Archived from the original on 1 March 2021. Retrieved 5 March 2021.\\n\\n^ a b \"TIOBE Index\". TIOBE. Archived from the original on 25 February 2018. Retrieved 3 January 2023. The TIOBE Programming Community index is an indicator of the popularity of programming languages Updated as required.\\n\\n^ \"PYPL PopularitY of Programming Language index\". pypl.github.io. Archived from the original on 14 March 2017. Retrieved 26 March 2021.\\n\\n^ a b Venners, Bill (13 January 2003). \"The Making of Python\". Artima Developer. Artima. Archived from the original on 1 September 2016. Retrieved 22 March 2007.\\n\\n^ van Rossum, Guido (29 August 2000). \"SETL (was: Lukewarm about range literals)\". Python-Dev (Mailing list). Archived from the original on 14 July 2018. Retrieved 13 March 2011.\\n\\n^ van Rossum, Guido (20 January 2009). \"A Brief Timeline of Python\". The History of Python. Archived from the original on 5 June 2020. Retrieved 20 January 2009.\\n\\n^ Fairchild, Carlie (12 July 2018). \"Guido van Rossum Stepping Down from Role as Python\\'s Benevolent Dictator For Life\". Linux Journal. Archived from the original on 13 July 2018. Retrieved 13 July 2018.\\n\\n^ \"PEP 8100\". Python Software Foundation. Archived from the original on 4 June 2020. Retrieved 4 May 2019.\\n\\n^ \"PEP 13 – Python Language Governance\". Python.org. Archived from the original on 27 May 2021. Retrieved 25 August 2021.\\n\\n^ Kuchling, A. M.; Zadka, Moshe (16 October 2000). \"What\\'s New in Python 2.0\". Python Software Foundation. Archived from the original on 23 October 2012. Retrieved 11 February 2012.\\n\\n^ van Rossum, Guido (5 April 2006). \"PEP 3000\\xa0– Python 3000\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 3 March 2016. Retrieved 27 June 2009.\\n\\n^ \"2to3 – Automated Python 2 to 3 code translation\". docs.python.org. Archived from the original on 4 June 2020. Retrieved 2 February 2021.\\n\\n^ \"PEP 373 – Python 2.7 Release Schedule\". python.org. Archived from the original on 19 May 2020. Retrieved 9 January 2017.\\n\\n^ \"PEP 466 – Network Security Enhancements for Python 2.7.x\". python.org. Archived from the original on 4 June 2020. Retrieved 9 January 2017.\\n\\n^ \"Sunsetting Python 2\". Python.org. Archived from the original on 12 January 2020. Retrieved 22 September 2019.\\n\\n^ \"PEP 373 – Python 2.7 Release Schedule\". Python.org. Archived from the original on 13 January 2020. Retrieved 22 September 2019.\\n\\n^ \"Python Release Python 3.7.17\". Python.org. Archived from the original on 31 July 2023. Retrieved 18 August 2023.\\n\\n^ mattip (25 December 2023). \"PyPy v7.3.14 release\". PyPy. Archived from the original on 5 January 2024. Retrieved 5 January 2024.\\n\\n^ \"CVE-2021-3177\". Red Hat Customer Portal. Archived from the original on 6 March 2021. Retrieved 26 February 2021.\\n\\n^ \"CVE-2021-3177\". CVE. Archived from the original on 27 February 2021. Retrieved 26 February 2021.\\n\\n^ \"CVE-2021-23336\". CVE. Archived from the original on 24 February 2021. Retrieved 26 February 2021.\\n\\n^ Langa, Łukasz (24 March 2022). \"Python 3.10.4 and 3.9.12 are now available out of schedule\". Python Insider. Archived from the original on 21 April 2022. Retrieved 19 April 2022.\\n\\n^ Langa, Łukasz (16 March 2022). \"Python 3.10.3, 3.9.11, 3.8.13, and 3.7.13 are now available with security content\". Python Insider. Archived from the original on 17 April 2022. Retrieved 19 April 2022.\\n\\n^ Langa, Łukasz (17 May 2022). \"Python 3.9.13 is now available\". Python Insider. Archived from the original on 17 May 2022. Retrieved 21 May 2022.\\n\\n^ Langa, Łukasz (7 September 2022). \"Python releases 3.10.7, 3.9.14, 3.8.14, and 3.7.14 are now available\". Python Insider. Archived from the original on 13 September 2022. Retrieved 16 September 2022.\\n\\n^ \"CVE-2020-10735\". CVE. Archived from the original on 20 September 2022. Retrieved 16 September 2022.\\n\\n^ \"Built-in Types\".\\n\\n^ corbet (24 October 2022). \"Python 3.11 released [LWN.net]\". lwn.net. Retrieved 15 November 2022.\\n\\n^ \"What\\'s New In Python 3.13\". Python documentation. Retrieved 30 April 2024.\\n\\n^ \"PEP 667 – Consistent views of namespaces | peps.python.org\". Python Enhancement Proposals (PEPs). Retrieved 7 October 2024.\\n\\n^ \"Status of Python versions\". Python Developer\\'s Guide. Retrieved 7 October 2024.\\n\\n^ Wouters, Thomas (9 April 2024). \"Python Insider: Python 3.12.3 and 3.13.0a6 released\". Python Insider. Retrieved 29 April 2024.\\n\\n^ \"PEP 594 – Removing dead batteries from the standard library\". Python Enhancement Proposals. Python Softtware Foundation. 20 May 2019.\\n\\n^ Hugo (15 October 2024). \"Python Insider: Python 3.14.0 alpha 1 is now available\". Python Insider. Retrieved 16 October 2024.\\n\\n^ \"PEP 649 – Deferred Evaluation Of Annotations Using Descriptors | peps.python.org\". Python Enhancement Proposals (PEPs). Retrieved 16 October 2024.\\n\\n^ The Cain Gang Ltd. \"Python Metaclasses: Who? Why? When?\" (PDF). Archived from the original (PDF) on 30 May 2009. Retrieved 27 June 2009.\\n\\n^ \"3.3. Special method names\". The Python Language Reference. Python Software Foundation. Archived from the original on 15 December 2018. Retrieved 27 June 2009.\\n\\n^ \"PyDBC: method preconditions, method postconditions and class invariants for Python\". Archived from the original on 23 November 2019. Retrieved 24 September 2011.\\n\\n^ \"Contracts for Python\". Archived from the original on 15 June 2020. Retrieved 24 September 2011.\\n\\n^ \"PyDatalog\". Archived from the original on 13 June 2020. Retrieved 22 July 2012.\\n\\n^ \"Glue It All Together With Python\". Python.org. Retrieved 30 September 2024.\\n\\n^ \"Extending and Embedding the Python Interpreter: Reference Counts\". Docs.python.org. Archived from the original on 18 October 2012. Retrieved 5 June 2020. Since Python makes heavy use of malloc() and free(), it needs a strategy to avoid memory leaks as well as the use of freed memory. The chosen method is called reference counting.\\n\\n^ a b Hettinger, Raymond (30 January 2002). \"PEP 289\\xa0– Generator Expressions\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 14 June 2020. Retrieved 19 February 2012.\\n\\n^ \"6.5 itertools\\xa0– Functions creating iterators for efficient looping\". Docs.python.org. Archived from the original on 14 June 2020. Retrieved 22 November 2016.\\n\\n^ a b Peters, Tim (19 August 2004). \"PEP 20\\xa0– The Zen of Python\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 26 December 2018. Retrieved 24 November 2008.\\n\\n^ Lutz, Mark (January 2022). \"Python Changes 2014+\". Learning Python. Archived from the original on 15 March 2024. Retrieved 25 February 2024.\\n\\n^ \"Confusion regarding a rule in The Zen of Python\". Python Help - Discussions on Python.org. 3 May 2022. Archived from the original on 25 February 2024. Retrieved 25 February 2024.\\n\\n^ Ambi, Chetan (4 July 2021). \"The Most Controversial Python Walrus Operator\". Python Simplified. Archived from the original on 27 August 2023. Retrieved 5 February 2024.\\n\\n^ Grifski, Jeremy (24 May 2020). \"The Controversy Behind The Walrus Operator in Python\". The Renegade Coder. Archived from the original on 28 December 2023. Retrieved 25 February 2024.\\n\\n^ Bader, Dan. \"Python String Formatting Best Practices\". Real Python. Archived from the original on 18 February 2024. Retrieved 25 February 2024.\\n\\n^ Martelli, Alex; Ravenscroft, Anna; Ascher, David (2005). Python Cookbook, 2nd Edition. O\\'Reilly Media. p.\\xa0230. ISBN\\xa0978-0-596-00797-3. Archived from the original on 23 February 2020. Retrieved 14 November 2015.\\n\\n^ \"Python Culture\". ebeab. 21 January 2014. Archived from the original on 30 January 2014.\\n\\n^ a b \"Transpiling Python to Julia using PyJL\" (PDF). Archived (PDF) from the original on 19 November 2023. Retrieved 20 September 2023. After manually modifying one line of code by specifying the necessary type information, we obtained a speedup of 52.6×, making the translated Julia code 19.5× faster than the original Python code.\\n\\n^ \"Why is it called Python?\". General Python FAQ. Docs.python.org. Archived from the original on 24 October 2012. Retrieved 3 January 2023.\\n\\n^ \"15 Ways Python Is a Powerful Force on the Web\". Archived from the original on 11 May 2019. Retrieved 3 July 2018.\\n\\n^ \"pprint – Data pretty printer – Python 3.11.0 documentation\". docs.python.org. Archived from the original on 22 January 2021. Retrieved 5 November 2022. stuff=[\\'spam\\', \\'eggs\\', \\'lumberjack\\', \\'knights\\', \\'ni\\']\\n\\n^ \"Code Style – The Hitchhiker\\'s Guide to Python\". docs.python-guide.org. Archived from the original on 27 January 2021. Retrieved 20 January 2021.\\n\\n^ \"Is Python a good language for beginning programmers?\". General Python FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 21 March 2007.\\n\\n^ \"Myths about indentation in Python\". Secnetix.de. Archived from the original on 18 February 2018. Retrieved 19 April 2011.\\n\\n^ Guttag, John V. (12 August 2016). Introduction to Computation and Programming Using Python: With Application to Understanding Data. MIT Press. ISBN\\xa0978-0-262-52962-4.\\n\\n^ \"PEP 8 – Style Guide for Python Code\". Python.org. Archived from the original on 17 April 2019. Retrieved 26 March 2019.\\n\\n^ \"8. Errors and Exceptions – Python 3.12.0a0 documentation\". docs.python.org. Archived from the original on 9 May 2022. Retrieved 9 May 2022.\\n\\n^ \"Highlights: Python 2.5\". Python.org. Archived from the original on 4 August 2019. Retrieved 20 March 2018.\\n\\n^ van Rossum, Guido (22 April 2009). \"Tail Recursion Elimination\". Neopythonic.blogspot.be. Archived from the original on 19 May 2018. Retrieved 3 December 2012.\\n\\n^ van Rossum, Guido (9 February 2006). \"Language Design Is Not Just Solving Puzzles\". Artima forums. Artima. Archived from the original on 17 January 2020. Retrieved 21 March 2007.\\n\\n^ van Rossum, Guido; Eby, Phillip J. (10 May 2005). \"PEP 342\\xa0– Coroutines via Enhanced Generators\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 29 May 2020. Retrieved 19 February 2012.\\n\\n^ \"PEP 380\". Python.org. Archived from the original on 4 June 2020. Retrieved 3 December 2012.\\n\\n^ \"division\". python.org. Archived from the original on 20 July 2006. Retrieved 30 July 2014.\\n\\n^ \"PEP 0465 – A dedicated infix operator for matrix multiplication\". python.org. Archived from the original on 4 June 2020. Retrieved 1 January 2016.\\n\\n^ \"Python 3.5.1 Release and Changelog\". python.org. Archived from the original on 14 May 2020. Retrieved 1 January 2016.\\n\\n^ \"What\\'s New in Python 3.8\". Archived from the original on 8 June 2020. Retrieved 14 October 2019.\\n\\n^ van Rossum, Guido; Hettinger, Raymond (7 February 2003). \"PEP 308\\xa0– Conditional Expressions\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 13 March 2016. Retrieved 13 July 2011.\\n\\n^ \"4. Built-in Types – Python 3.6.3rc1 documentation\". python.org. Archived from the original on 14 June 2020. Retrieved 1 October 2017.\\n\\n^ \"5.3. Tuples and Sequences – Python 3.7.1rc2 documentation\". python.org. Archived from the original on 10 June 2020. Retrieved 17 October 2018.\\n\\n^ a b \"PEP 498 – Literal String Interpolation\". python.org. Archived from the original on 15 June 2020. Retrieved 8 March 2017.\\n\\n^ \"Why must \\'self\\' be used explicitly in method definitions and calls?\". Design and History FAQ. Python Software Foundation. Archived from the original on 24 October 2012. Retrieved 19 February 2012.\\n\\n^ Sweigart, Al (2020). Beyond the Basic Stuff with Python: Best Practices for Writing Clean Code. No Starch Press. p.\\xa0322. ISBN\\xa0978-1-59327-966-0. Archived from the original on 13 August 2021. Retrieved 7 July 2021.\\n\\n^ \"The Python Language Reference, section 3.3. New-style and classic classes, for release 2.7.1\". Archived from the original on 26 October 2012. Retrieved 12 January 2011.\\n\\n^ \"PEP 484 – Type Hints | peps.python.org\". peps.python.org. Archived from the original on 27 November 2023. Retrieved 29 November 2023.\\n\\n^ \"typing — Support for type hints\". Python documentation. Python Software Foundation. Archived from the original on 21 February 2020. Retrieved 22 December 2023.\\n\\n^ \"mypy – Optional Static Typing for Python\". Archived from the original on 6 June 2020. Retrieved 28 January 2017.\\n\\n^ \"Introduction\". mypyc.readthedocs.io. Archived from the original on 22 December 2023. Retrieved 22 December 2023.\\n\\n^ \"15. Floating Point Arithmetic: Issues and Limitations – Python 3.8.3 documentation\". docs.python.org. Archived from the original on 6 June 2020. Retrieved 6 June 2020. Almost all machines today (November 2000) use IEEE-754 floating point arithmetic, and almost all platforms map Python floats to IEEE-754 \"double precision\".\\n\\n^ Zadka, Moshe; van Rossum, Guido (11 March 2001). \"PEP 237\\xa0– Unifying Long Integers and Integers\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 28 May 2020. Retrieved 24 September 2011.\\n\\n^ \"Built-in Types\". Archived from the original on 14 June 2020. Retrieved 3 October 2019.\\n\\n^ \"PEP 465 – A dedicated infix operator for matrix multiplication\". python.org. Archived from the original on 29 May 2020. Retrieved 3 July 2018.\\n\\n^ a b Zadka, Moshe; van Rossum, Guido (11 March 2001). \"PEP 238\\xa0– Changing the Division Operator\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 28 May 2020. Retrieved 23 October 2013.\\n\\n^ \"Why Python\\'s Integer Division Floors\". 24 August 2010. Archived from the original on 5 June 2020. Retrieved 25 August 2010.\\n\\n^ \"round\", The Python standard library, release 3.2, §2: Built-in functions, archived from the original on 25 October 2012, retrieved 14 August 2011\\n\\n^ \"round\", The Python standard library, release 2.7, §2: Built-in functions, archived from the original on 27 October 2012, retrieved 14 August 2011\\n\\n^ Beazley, David M. (2009). Python Essential Reference (4th\\xa0ed.). Addison-Wesley Professional. p.\\xa066. ISBN\\xa09780672329784.\\n\\n^ Kernighan, Brian W.; Ritchie, Dennis M. (1988). The C Programming Language (2nd\\xa0ed.). p.\\xa0206.\\n\\n^ a b Batista, Facundo (17 October 2003). \"PEP 327\\xa0– Decimal Data Type\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 4 June 2020. Retrieved 24 November 2008.\\n\\n^ \"What\\'s New in Python 2.6\". Python v2.6.9 documentation. 29 October 2013. Archived from the original on 23 December 2019. Retrieved 26 September 2015.\\n\\n^ \"10 Reasons Python Rocks for Research (And a Few Reasons it Doesn\\'t) – Hoyt Koepke\". University of Washington Department of Statistics. Archived from the original on 31 May 2020. Retrieved 3 February 2019.\\n\\n^ Shell, Scott (17 June 2014). \"An introduction to Python for scientific computing\" (PDF). Archived (PDF) from the original on 4 February 2019. Retrieved 3 February 2019.\\n\\n^ Piotrowski, Przemyslaw (July 2006). \"Build a Rapid Web Development Environment for Python Server Pages and Oracle\". Oracle Technology Network. Oracle. Archived from the original on 2 April 2019. Retrieved 12 March 2012.\\n\\n^ Eby, Phillip J. (7 December 2003). \"PEP 333\\xa0– Python Web Server Gateway Interface v1.0\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 14 June 2020. Retrieved 19 February 2012.\\n\\n^ \"PyPI\". PyPI. 17 March 2024. Archived from the original on 17 March 2024.\\n\\n^ Enthought, Canopy. \"Canopy\". www.enthought.com. Archived from the original on 15 July 2017. Retrieved 20 August 2016.\\n\\n^ \"PEP 7 – Style Guide for C Code | peps.python.org\". peps.python.org. Archived from the original on 24 April 2022. Retrieved 28 April 2022.\\n\\n^ \"4. Building C and C++ Extensions – Python 3.9.2 documentation\". docs.python.org. Archived from the original on 3 March 2021. Retrieved 1 March 2021.\\n\\n^ van Rossum, Guido (5 June 2001). \"PEP 7\\xa0– Style Guide for C Code\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 1 June 2020. Retrieved 24 November 2008.\\n\\n^ \"CPython byte code\". Docs.python.org. Archived from the original on 5 June 2020. Retrieved 16 February 2016.\\n\\n^ \"Python 2.5 internals\" (PDF). Archived (PDF) from the original on 6 August 2012. Retrieved 19 April 2011.\\n\\n^ \"Changelog – Python 3.9.0 documentation\". docs.python.org. Archived from the original on 7 February 2021. Retrieved 8 February 2021.\\n\\n^ \"Download Python\". Python.org. Archived from the original on 8 December 2020. Retrieved 13 December 2020.\\n\\n^ \"history [vmspython]\". www.vmspython.org. Archived from the original on 2 December 2020. Retrieved 4 December 2020.\\n\\n^ \"An Interview with Guido van Rossum\". Oreilly.com. Archived from the original on 16 July 2014. Retrieved 24 November 2008.\\n\\n^ \"Download Python for Other Platforms\". Python.org. Archived from the original on 27 November 2020. Retrieved 4 December 2020.\\n\\n^ \"PyPy compatibility\". Pypy.org. Archived from the original on 6 June 2020. Retrieved 3 December 2012.\\n\\n^ Team, The PyPy (28 December 2019). \"Download and Install\". PyPy. Archived from the original on 8 January 2022. Retrieved 8 January 2022.\\n\\n^ \"speed comparison between CPython and Pypy\". Speed.pypy.org. Archived from the original on 10 May 2021. Retrieved 3 December 2012.\\n\\n^ \"Codon: Differences with Python\". Archived from the original on 25 May 2023. Retrieved 28 August 2023.\\n\\n^ Lawson, Loraine (14 March 2023). \"MIT-Created Compiler Speeds up Python Code\". The New Stack. Archived from the original on 6 April 2023. Retrieved 28 August 2023.\\n\\n^ \"Application-level Stackless features – PyPy 2.0.2 documentation\". Doc.pypy.org. Archived from the original on 4 June 2020. Retrieved 17 July 2013.\\n\\n^ \"Python-for-EV3\". LEGO Education. Archived from the original on 7 June 2020. Retrieved 17 April 2019.\\n\\n^ Yegulalp, Serdar (29 October 2020). \"Pyston returns from the dead to speed Python\". InfoWorld. Archived from the original on 27 January 2021. Retrieved 26 January 2021.\\n\\n^ \"cinder: Instagram\\'s performance-oriented fork of CPython\". GitHub. Archived from the original on 4 May 2021. Retrieved 4 May 2021.\\n\\n^ Aroca, Rafael (7 August 2021). \"Snek Lang: feels like Python on Arduinos\". Yet Another Technology Blog. Archived from the original on 5 January 2024. Retrieved 4 January 2024.\\n\\n^ Aufranc (CNXSoft), Jean-Luc (16 January 2020). \"Snekboard Controls LEGO Power Functions with CircuitPython or Snek Programming Languages (Crowdfunding) – CNX Software\". CNX Software – Embedded Systems News. Archived from the original on 5 January 2024. Retrieved 4 January 2024.\\n\\n^ Kennedy (@mkennedy), Michael. \"Ready to find out if you\\'re git famous?\". pythonbytes.fm. Archived from the original on 5 January 2024. Retrieved 4 January 2024.\\n\\n^ Packard, Keith (20 December 2022). \"The Snek Programming Language: A Python-inspired Embedded Computing Language\" (PDF). Archived (PDF) from the original on 4 January 2024. Retrieved 4 January 2024.\\n\\n^ \"Plans for optimizing Python\". Google Project Hosting. 15 December 2009. Archived from the original on 11 April 2016. Retrieved 24 September 2011.\\n\\n^ \"Python on the Nokia N900\". Stochastic Geometry. 29 April 2010. Archived from the original on 20 June 2019. Retrieved 9 July 2015.\\n\\n^ \"Brython\". brython.info. Archived from the original on 3 August 2018. Retrieved 21 January 2021.\\n\\n^ \"Transcrypt – Python in the browser\". transcrypt.org. Archived from the original on 19 August 2018. Retrieved 22 December 2020.\\n\\n^ \"Transcrypt: Anatomy of a Python to JavaScript Compiler\". InfoQ. Archived from the original on 5 December 2020. Retrieved 20 January 2021.\\n\\n^ \"Nuitka Home | Nuitka Home\". nuitka.net. Archived from the original on 30 May 2020. Retrieved 18 August 2017.\\n\\n^ Guelton, Serge; Brunet, Pierrick; Amini, Mehdi; Merlini, Adrien; Corbillon, Xavier; Raynaud, Alan (16 March 2015). \"Pythran: enabling static optimization of scientific Python programs\". Computational Science & Discovery. 8 (1). IOP Publishing: 014001. Bibcode:2015CS&D....8a4001G. doi:10.1088/1749-4680/8/1/014001. ISSN\\xa01749-4699.\\n\\n^ \"The Python → 11l → C++ transpiler\". Archived from the original on 24 September 2022. Retrieved 17 July 2022.\\n\\n^ \"google/grumpy\". 10 April 2020. Archived from the original on 15 April 2020. Retrieved 25 March 2020 – via GitHub.\\n\\n^ \"Projects\". opensource.google. Archived from the original on 24 April 2020. Retrieved 25 March 2020.\\n\\n^ Francisco, Thomas Claburn in San. \"Google\\'s Grumpy code makes Python Go\". www.theregister.com. Archived from the original on 7 March 2021. Retrieved 20 January 2021.\\n\\n^ \"GitHub – IronLanguages/ironpython3: Implementation of Python 3.x for .NET Framework that is built on top of the Dynamic Language Runtime\". GitHub. Archived from the original on 28 September 2021.\\n\\n^ \"IronPython.net /\". ironpython.net. Archived from the original on 17 April 2021.\\n\\n^ \"Jython FAQ\". www.jython.org. Archived from the original on 22 April 2021. Retrieved 22 April 2021.\\n\\n^ Murri, Riccardo (2013). Performance of Python runtimes on a non-numeric scientific code. European Conference on Python in Science (EuroSciPy). arXiv:1404.6388. Bibcode:2014arXiv1404.6388M.\\n\\n^ \"The Computer Language Benchmarks Game\". Archived from the original on 14 June 2020. Retrieved 30 April 2020.\\n\\n^ a b Warsaw, Barry; Hylton, Jeremy; Goodger, David (13 June 2000). \"PEP 1\\xa0– PEP Purpose and Guidelines\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 6 June 2020. Retrieved 19 April 2011.\\n\\n^ \"PEP 8 – Style Guide for Python Code\". Python.org. Archived from the original on 17 April 2019. Retrieved 26 March 2019.\\n\\n^ Cannon, Brett. \"Guido, Some Guys, and a Mailing List: How Python is Developed\". python.org. Python Software Foundation. Archived from the original on 1 June 2009. Retrieved 27 June 2009.\\n\\n^ \"Moving Python\\'s bugs to GitHub [LWN.net]\". Archived from the original on 2 October 2022. Retrieved 2 October 2022.\\n\\n^ \"Python Developer\\'s Guide – Python Developer\\'s Guide\". devguide.python.org. Archived from the original on 9 November 2020. Retrieved 17 December 2019.\\n\\n^ Hughes, Owen (24 May 2021). \"Programming languages: Why Python 4.0 might never arrive, according to its creator\". TechRepublic. Archived from the original on 14 July 2022. Retrieved 16 May 2022.\\n\\n^ \"PEP 602 – Annual Release Cycle for Python\". Python.org. Archived from the original on 14 June 2020. Retrieved 6 November 2019.\\n\\n^ \"Changing the Python release cadence [LWN.net]\". lwn.net. Archived from the original on 6 November 2019. Retrieved 6 November 2019.\\n\\n^ Norwitz, Neal (8 April 2002). \"[Python-Dev] Release Schedules (was Stability & change)\". Archived from the original on 15 December 2018. Retrieved 27 June 2009.\\n\\n^ a b Aahz; Baxter, Anthony (15 March 2001). \"PEP 6\\xa0– Bug Fix Releases\". Python Enhancement Proposals. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 27 June 2009.\\n\\n^ \"Python Buildbot\". Python Developer\\'s Guide. Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 24 September 2011.\\n\\n^ \"1. Extending Python with C or C++ – Python 3.9.1 documentation\". docs.python.org. Archived from the original on 23 June 2020. Retrieved 14 February 2021.\\n\\n^ \"PEP 623 – Remove wstr from Unicode\". Python.org. Archived from the original on 5 March 2021. Retrieved 14 February 2021.\\n\\n^ \"PEP 634 – Structural Pattern Matching: Specification\". Python.org. Archived from the original on 6 May 2021. Retrieved 14 February 2021.\\n\\n^ \"Documentation Tools\". Python.org. Archived from the original on 11 November 2020. Retrieved 22 March 2021.\\n\\n^ a b \"Whetting Your Appetite\". The Python Tutorial. Python Software Foundation. Archived from the original on 26 October 2012. Retrieved 20 February 2012.\\n\\n^ \"In Python, should I use else after a return in an if block?\". Stack Overflow. Stack Exchange. 17 February 2011. Archived from the original on 20 June 2019. Retrieved 6 May 2011.\\n\\n^ Lutz, Mark (2009). Learning Python: Powerful Object-Oriented Programming. O\\'Reilly Media, Inc. p.\\xa017. ISBN\\xa09781449379322. Archived from the original on 17 July 2017. Retrieved 9 May 2017.\\n\\n^ Fehily, Chris (2002). Python. Peachpit Press. p.\\xa0xv. ISBN\\xa09780201748840. Archived from the original on 17 July 2017. Retrieved 9 May 2017.\\n\\n^ Lubanovic, Bill (2014). Introducing Python. Sebastopol, CA\\xa0: O\\'Reilly Media. p.\\xa0305. ISBN\\xa0978-1-4493-5936-2. Retrieved 31 July 2023.\\n\\n^ Blake, Troy (18 January 2021). \"TIOBE Index for January 2021\". Technology News and Information by SeniorDBA. Archived from the original on 21 March 2021. Retrieved 26 February 2021.\\n\\n^ \"Quotes about Python\". Python Software Foundation. Archived from the original on 3 June 2020. Retrieved 8 January 2012.\\n\\n^ \"Organizations Using Python\". Python Software Foundation. Archived from the original on 21 August 2018. Retrieved 15 January 2009.\\n\\n^ \"Python\\xa0: the holy grail of programming\". CERN Bulletin (31/2006). CERN Publications. 31 July 2006. Archived from the original on 15 January 2013. Retrieved 11 February 2012.\\n\\n^ Shafer, Daniel G. (17 January 2003). \"Python Streamlines Space Shuttle Mission Design\". Python Software Foundation. Archived from the original on 5 June 2020. Retrieved 24 November 2008.\\n\\n^ \"Tornado: Facebook\\'s Real-Time Web Framework for Python – Facebook for Developers\". Facebook for Developers. Archived from the original on 19 February 2019. Retrieved 19 June 2018.\\n\\n^ \"What Powers Instagram: Hundreds of Instances, Dozens of Technologies\". Instagram Engineering. 11 December 2016. Archived from the original on 15 June 2020. Retrieved 27 May 2019.\\n\\n^ \"How we use Python at Spotify\". Spotify Labs. 20 March 2013. Archived from the original on 10 June 2020. Retrieved 25 July 2018.\\n\\n^ Fortenberry, Tim (17 January 2003). \"Industrial Light & Magic Runs on Python\". Python Software Foundation. Archived from the original on 6 June 2020. Retrieved 11 February 2012.\\n\\n^ Taft, Darryl K. (5 March 2007). \"Python Slithers into Systems\". eWeek.com. Ziff Davis Holdings. Archived from the original on 13 August 2021. Retrieved 24 September 2011.\\n\\n^ GitHub – reddit-archive/reddit: historical code from reddit.com., The Reddit Archives, archived from the original on 1 June 2020, retrieved 20 March 2019\\n\\n^ \"Real time communication at scale with Elixir at Discord\". 8 October 2020.\\n\\n^ \"What Programming Language is Baidu Built In?\". 5 July 2018.\\n\\n^ \"Usage statistics and market share of Python for websites\". 2012. Archived from the original on 13 August 2021. Retrieved 18 December 2012.\\n\\n^ Oliphant, Travis (2007). \"Python for Scientific Computing\". Computing in Science and Engineering. 9 (3): 10–20. Bibcode:2007CSE.....9c..10O. CiteSeerX\\xa010.1.1.474.6460. doi:10.1109/MCSE.2007.58. ISSN\\xa01521-9615. S2CID\\xa0206457124. Archived from the original on 15 June 2020. Retrieved 10 April 2015.\\n\\n^ Millman, K. Jarrod; Aivazis, Michael (2011). \"Python for Scientists and Engineers\". Computing in Science and Engineering. 13 (2): 9–12. Bibcode:2011CSE....13b...9M. doi:10.1109/MCSE.2011.36. Archived from the original on 19 February 2019. Retrieved 7 July 2014.\\n\\n^ Science education with SageMath, Innovative Computing in Science Education, archived from the original on 15 June 2020, retrieved 22 April 2019\\n\\n^ \"OpenCV: OpenCV-Python Tutorials\". docs.opencv.org. Archived from the original on 23 September 2020. Retrieved 14 September 2020.\\n\\n^ Dean, Jeff; Monga, Rajat; et\\xa0al. (9 November 2015). \"TensorFlow: Large-scale machine learning on heterogeneous systems\" (PDF). TensorFlow.org. Google Research. Archived (PDF) from the original on 20 November 2015. Retrieved 10 November 2015.\\n\\n^ Piatetsky, Gregory. \"Python eats away at R: Top Software for Analytics, Data Science, Machine Learning in 2018: Trends and Analysis\". KDnuggets. Archived from the original on 15 November 2019. Retrieved 30 May 2018.\\n\\n^ \"Who is using scikit-learn? – scikit-learn 0.20.1 documentation\". scikit-learn.org. Archived from the original on 6 May 2020. Retrieved 30 November 2018.\\n\\n^ Jouppi, Norm. \"Google supercharges machine learning tasks with TPU custom chip\". Google Cloud Platform Blog. Archived from the original on 18 May 2016. Retrieved 19 May 2016.\\n\\n^ De Raedt, Luc; Kimmig, Angelika (2015). \"Probabilistic (logic) programming concepts\". Machine Learning. 100 (1): 5–47. doi:10.1007/s10994-015-5494-z. S2CID\\xa03166992.\\n\\n^ \"Natural Language Toolkit – NLTK 3.5b1 documentation\". www.nltk.org. Archived from the original on 13 June 2020. Retrieved 10 April 2020.\\n\\n^ Andersen, C. and Swift, T., 2023. The Janus System: a bridge to new prolog applications. In Prolog: The Next 50 Years (pp. 93–104). Cham: Springer Nature Switzerland.\\n\\n^ \"SWI-Prolog Python interface\". Archived from the original on 15 March 2024. Retrieved 15 March 2024.\\n\\n^ Tarau, P., 2023. Reflections on automation, learnability and expressiveness in logic-based programming languages. In Prolog: The Next 50 Years (pp. 359–371). Cham: Springer Nature Switzerland.\\n\\n^ \"Tkinter — Python interface to TCL/Tk\". Archived from the original on 18 October 2012. Retrieved 9 June 2023.\\n\\n^ \"Python Tkinter Tutorial\". 3 June 2020. Archived from the original on 9 June 2023. Retrieved 9 June 2023.\\n\\n^ \"Installers for GIMP for Windows – Frequently Asked Questions\". 26 July 2013. Archived from the original on 17 July 2013. Retrieved 26 July 2013.\\n\\n^ \"jasc psp9components\". Archived from the original on 19 March 2008.\\n\\n^ \"About getting started with writing geoprocessing scripts\". ArcGIS Desktop Help 9.2. Environmental Systems Research Institute. 17 November 2006. Archived from the original on 5 June 2020. Retrieved 11 February 2012.\\n\\n^ CCP porkbelly (24 August 2010). \"Stackless Python 2.7\". EVE Community Dev Blogs. CCP Games. Archived from the original on 11 January 2014. Retrieved 11 January 2014. As you may know, EVE has at its core the programming language known as Stackless Python.\\n\\n^ Caudill, Barry (20 September 2005). \"Modding Sid Meier\\'s Civilization IV\". Sid Meier\\'s Civilization IV Developer Blog. Firaxis Games. Archived from the original on 2 December 2010. we created three levels of tools ... The next level offers Python and XML support, letting modders with more experience manipulate the game world and everything in it.\\n\\n^ \"Python Language Guide (v1.0)\". Google Documents List Data API v1.0. Archived from the original on 15 July 2010.\\n\\n^ \"Python Setup and Usage\". Python Software Foundation. Archived from the original on 17 June 2020. Retrieved 10 January 2020.\\n\\n^ \"Immunity: Knowing You\\'re Secure\". Archived from the original on 16 February 2009.\\n\\n^ \"Core Security\". Core Security. Archived from the original on 9 June 2020. Retrieved 10 April 2020.\\n\\n^ \"What is Sugar?\". Sugar Labs. Archived from the original on 9 January 2009. Retrieved 11 February 2012.\\n\\n^ \"4.0 New Features and Fixes\". LibreOffice.org. The Document Foundation. 2013. Archived from the original on 9 February 2014. Retrieved 25 February 2013.\\n\\n^ \"Gotchas for Python Users\". boo.codehaus.org. Codehaus Foundation. Archived from the original on 11 December 2008. Retrieved 24 November 2008.\\n\\n^ Esterbrook, Charles. \"Acknowledgements\". cobra-language.com. Cobra Language. Archived from the original on 8 February 2008. Retrieved 7 April 2010.\\n\\n^ \"Proposals: iterators and generators [ES4 Wiki]\". wiki.ecmascript.org. Archived from the original on 20 October 2007. Retrieved 24 November 2008.\\n\\n^ \"Frequently asked questions\". Godot Engine documentation. Archived from the original on 28 April 2021. Retrieved 10 May 2021.\\n\\n^ Kincaid, Jason (10 November 2009). \"Google\\'s Go: A New Programming Language That\\'s Python Meets C++\". TechCrunch. Archived from the original on 18 January 2010. Retrieved 29 January 2010.\\n\\n^ Strachan, James (29 August 2003). \"Groovy\\xa0– the birth of a new dynamic language for the Java platform\". Archived from the original on 5 April 2007. Retrieved 11 June 2007.\\n\\n^ \"Modular Docs – Why Mojo\". docs.modular.com. Archived from the original on 5 May 2023. Retrieved 5 May 2023. Mojo as a member of the Python family [..] Embracing Python massively simplifies our design efforts, because most of the syntax is already specified.  [..] we decided that the right long-term goal for Mojo is to provide a superset of Python (i.e. be compatible with existing programs) and to embrace the CPython immediately for long-tail ecosystem enablement. To a Python programmer, we expect and hope that Mojo will be immediately familiar, while also providing new tools for developing systems-level code that enable you to do things that Python falls back to C and C++ for.\\n\\n^ Spencer, Michael (4 May 2023). \"What is Mojo Programming Language?\". datasciencelearningcenter.substack.com. Archived from the original on 5 May 2023. Retrieved 5 May 2023.\\n\\n^ Yegulalp, Serdar (16 January 2017). \"Nim language draws from best of Python, Rust, Go, and Lisp\". InfoWorld. Archived from the original on 13 October 2018. Retrieved 7 June 2020. Nim\\'s syntax is strongly reminiscent of Python\\'s, as it uses indented code blocks and some of the same syntax (such as the way if/elif/then/else blocks are constructed).\\n\\n^ \"An Interview with the Creator of Ruby\". Linuxdevcenter.com. Archived from the original on 28 April 2018. Retrieved 3 December 2012.\\n\\n^ Lattner, Chris (3 June 2014). \"Chris Lattner\\'s Homepage\". Chris Lattner. Archived from the original on 22 December 2015. Retrieved 3 June 2014. I started work on the Swift Programming Language in July of 2010. I implemented much of the basic language structure, with only a few people knowing of its existence. A few other (amazing) people started contributing in earnest late in 2011, and it became a major focus for the Apple Developer Tools group in July 2013 [...] drawing ideas from Objective-C, Rust, Haskell, Ruby, Python, C#, CLU, and far too many others to list.\\n\\n^ Jalan, Nishant Aanjaney (10 November 2022). \"Programming in Kotlin\". CodeX. Retrieved 29 April 2024.\\n\\n^ Kupries, Andreas; Fellows, Donal K. (14 September 2000). \"TIP #3: TIP Format\". tcl.tk. Tcl Developer Xchange. Archived from the original on 13 July 2017. Retrieved 24 November 2008.\\n\\n^ Gustafsson, Per; Niskanen, Raimo (29 January 2007). \"EEP 1: EEP Purpose and Guidelines\". erlang.org. Archived from the original on 15 June 2020. Retrieved 19 April 2011.\\n\\n^ \"Swift Evolution Process\". Swift Programming Language Evolution repository on GitHub. 18 February 2020. Archived from the original on 27 April 2020. Retrieved 27 April 2020.\\n\\n\\nSources[edit]\\n\"Python for Artificial Intelligence\". Python Wiki. 19 July 2012. Archived from the original on 1 November 2012. Retrieved 3 December 2012.\\nPaine, Jocelyn, ed. (August 2005). \"AI in Python\". AI Expert Newsletter. Amzi!. Archived from the original on 26 March 2012. Retrieved 11 February 2012.\\n\"PyAIML 0.8.5\\xa0: Python Package Index\". Pypi.python.org. Retrieved 17 July 2013.\\nRussell, Stuart J. & Norvig, Peter (2009). Artificial Intelligence: A Modern Approach (3rd\\xa0ed.). Upper Saddle River, NJ: Prentice Hall. ISBN\\xa0978-0-13-604259-4.\\nFurther reading[edit]\\nDowney, Allen B. (May 2012). Think Python: How to Think Like a Computer Scientist (version 1.6.6\\xa0ed.). Cambridge University Press. ISBN\\xa0978-0-521-72596-5.\\nHamilton, Naomi (5 August 2008). \"The A-Z of Programming Languages: Python\". Computerworld. Archived from the original on 29 December 2008. Retrieved 31 March 2010.\\nLutz, Mark (2013). Learning Python (5th\\xa0ed.). O\\'Reilly Media. ISBN\\xa0978-0-596-15806-4.\\nSummerfield, Mark (2009). Programming in Python 3 (2nd\\xa0ed.). Addison-Wesley Professional. ISBN\\xa0978-0-321-68056-3.\\nRamalho, Luciano (May 2022). Fluent Python. O\\'Reilly Media. ISBN\\xa0978-1-4920-5632-4.\\nExternal links[edit]\\n\\n\\nPython  at Wikipedia\\'s sister projects\\n\\nMedia from CommonsQuotations from WikiquoteTextbooks from WikibooksResources from WikiversityData from Wikidata\\n\\nOfficial website \\nvtePythonImplementations\\nCircuitPython\\nCLPython\\nCPython\\nCython\\nMicroPython\\nNumba\\nIronPython\\nJython\\nPsyco\\nPyPy\\nPython for S60\\nShed Skin\\nStackless Python\\nUnladen Swallow\\nmore...\\nIDEs\\neric\\nIDLE\\nNinja-IDE\\nPyCharm\\nPyDev\\nSpyder\\nmore...\\nTopics\\nWSGI\\nASGI\\nDesigner\\nGuido van Rossum\\n\\nSoftware (list)\\nPython Software Foundation\\nPython Conference (PyCon)\\n\\nvteProgramming languages\\nComparison\\nTimeline\\nHistory\\n\\nAda\\nALGOL\\nSimula\\nAPL\\nAssembly\\nBASIC\\nVisual Basic\\nclassic\\n.NET\\nC\\nC++\\nC#\\nCOBOL\\nErlang\\nForth\\nFortran\\nGo\\nHaskell\\nJava\\nJavaScript\\nJulia\\nKotlin\\nLisp\\nLua\\nMATLAB\\nML\\nPascal\\nObject Pascal\\nPerl\\nPHP\\nProlog\\nPython\\nR\\nRuby\\nRust\\nSQL\\nScratch\\nShell\\nSmalltalk\\nSwift\\nmore...\\n\\n Lists: Alphabetical\\nCategorical\\nGenerational\\nNon-English-based\\n Category\\n\\nvtePython web frameworks\\nBottle\\nCherryPy\\nCubicWeb\\nDjango\\nFastAPI\\nFlask\\nGrok\\nNevow\\nPylons\\nPyramid\\nQuixote\\nTACTIC\\nTornado\\nTurboGears\\nTwistedWeb\\nweb2py\\nZope 2\\nmore...\\n\\nvteDifferentiable computingGeneral\\nDifferentiable programming\\nInformation geometry\\nStatistical manifold\\nAutomatic differentiation\\nNeuromorphic engineering\\nPattern recognition\\nTensor calculus\\nComputational learning theory\\nInductive bias\\nConcepts\\nParameter\\nHyperparameter\\nLoss functions\\nRegression\\nBias–variance tradeoff\\nDouble descent\\nOverfitting\\nClustering\\nGradient descent\\nSGD\\nQuasi-Newton method\\nConjugate gradient descent\\nBackpropagation\\nAttention\\nConvolution\\nNormalization\\nBatchnorm\\nActivation\\nSoftmax\\nSigmoid\\nRectifier\\nGating\\nWeight initialization\\nRegularization\\nDatasets\\nAugmentation\\nReinforcement learning\\nQ-learning\\nSARSA\\nImitation\\nDiffusion\\nAutoregression\\nAdversary\\nHallucination\\nApplications\\nMachine learning\\nIn-context learning\\nArtificial neural network\\nDeep learning\\nScientific computing\\nArtificial Intelligence\\nLanguage model\\nLarge language model\\nNMT\\nHardware\\nIPU\\nTPU\\nVPU\\nMemristor\\nSpiNNaker\\nSoftware libraries\\nTensorFlow\\nPyTorch\\nKeras\\nscikit-learn\\nTheano\\nJAX\\nFlux.jl\\nMindSpore\\nImplementationsAudio–visual\\nAlexNet\\nWaveNet\\nHuman image synthesis\\nHWR\\nOCR\\nSpeech synthesis\\nSpeech recognition\\nFacial recognition\\nAlphaFold\\nText-to-image models\\nLatent diffusion model\\nDALL-E\\nMidjourney\\nStable Diffusion\\nText-to-video models\\nSora\\nVideoPoet\\nWhisper\\nText\\nWord2vec\\nSeq2seq\\nGloVe\\nBERT\\nT5\\nLlama\\nChinchilla AI\\nPaLM\\nGPT\\n1\\nJ\\n2\\n3\\nChatGPT\\n4\\nClaude\\nGemini\\nLaMDA\\nBard\\nBLOOM\\nProject Debater\\nIBM Watson\\nIBM Watsonx\\nGranite\\nPanGu-Σ\\nDecisional\\nAlphaGo\\nAlphaZero\\nOpenAI Five\\nSelf-driving car\\nMuZero\\nAction selection\\nAuto-GPT\\nRobot control\\nPeople\\nFrank Rosenblatt\\nBernard Widrow\\nPaul Werbos\\nYoshua Bengio\\nAlex Graves\\nIan Goodfellow\\nStephen Grossberg\\nDemis Hassabis\\nGeoffrey Hinton\\nYann LeCun\\nFei-Fei Li\\nAndrew Ng\\nJürgen Schmidhuber\\nDavid Silver\\nIlya Sutskever\\nOrganizations\\nAnthropic\\nEleutherAI\\nGoogle DeepMind\\nHugging Face\\nOpenAI\\nMeta AI\\nMila\\nMIT CSAIL\\nHuawei\\nArchitectures\\nNeural Turing machine\\nDifferentiable neural computer\\nTransformer\\nVision transformer (ViT)\\nRecurrent neural network (RNN)\\nLong short-term memory (LSTM)\\nGated recurrent unit (GRU)\\nEcho state network\\nMultilayer perceptron (MLP)\\nConvolutional neural network (CNN)\\nResidual neural network (RNN)\\nHighway network\\nMamba\\nAutoencoder\\nVariational autoencoder (VAE)\\nGenerative adversarial network (GAN)\\nGraph neural network (GNN)\\n\\n Portals\\nComputer programming\\nTechnology\\n Categories\\nArtificial neural networks\\nMachine learning\\n\\nvteFree and open-source softwareGeneral\\nAlternative terms for free software\\nComparison of open-source and closed-source software\\nComparison of source-code-hosting facilities\\nFree software\\nFree software project directories\\nGratis versus libre\\nLong-term support\\nOpen-source software\\nOpen-source software development\\nOutline\\nTimeline\\nSoftwarepackages\\nAudio\\nBioinformatics\\nCodecs\\nConfiguration management\\nDrivers\\nGraphics\\nWireless\\nHealth\\nMathematics\\nOffice suites\\nOperating systems\\nRouting\\nTelevision\\nVideo games\\nWeb applications\\nE-commerce\\nAndroid apps\\niOS apps\\nCommercial\\nFormerly proprietary\\nFormerly open-source\\nCommunity\\nFree software movement\\nHistory\\nOpen-source-software movement\\nEvents\\nAdvocacy\\nOrganisations\\nFree Software Movement of India\\nFree Software Foundation\\nLicenses\\nAFL\\nApache\\nAPSL\\nArtistic\\nBeerware\\nBSD\\nCreative Commons\\nCDDL\\nEPL\\nFree Software Foundation\\nGNU GPL\\nGNU AGPL\\nGNU LGPL\\nISC\\nMIT\\nMPL\\nPython\\nPython Software Foundation License\\nShared Source Initiative\\nSleepycat\\nUnlicense\\nWTFPL\\nzlib\\nTypes and standards\\nComparison of licenses\\nContributor License Agreement\\nCopyleft\\nDebian Free Software Guidelines\\nDefinition of Free Cultural Works\\nFree license\\nThe Free Software Definition\\nThe Open Source Definition\\nOpen-source license\\nPermissive software license\\nPublic domain\\n\\nChallenges\\nDigital rights management\\nLicense proliferation\\nMozilla software rebranding\\nProprietary device drivers\\nProprietary firmware\\nProprietary software\\nSCO/Linux controversies\\nSoftware patents\\nSoftware security\\nTivoization\\nTrusted Computing\\nRelated topics\\nForking\\nGNU Manifesto\\nMicrosoft Open Specification Promise\\nOpen-core model\\nOpen-source hardware\\nShared Source Initiative\\nSource-available software\\nThe Cathedral and the Bazaar\\nRevolution OS\\n\\n Portal\\n Category\\n\\nvteStatistical softwarePublic domain\\nDataplot\\nEpi Info\\nCSPro\\nX-12-ARIMA\\nOpen-source\\nADMB\\nDAP\\ngretl\\nJASP\\nJAGS\\nJMulTi\\nJulia\\nJupyter (Julia, Python, R)\\nGNU Octave\\nOpenBUGS\\nOrange\\nPSPP\\nPython (statsmodels, PyMC3, IPython, IDLE)\\nR (RStudio)\\nSageMath\\nSimFiT\\nSOFA Statistics\\nStan\\nXLispStat\\nFreeware\\nBV4.1\\nCumFreq\\nSegReg\\nXploRe\\nWinBUGS\\nCommercialCross-platform\\nData Desk\\nGAUSS\\nGraphPad InStat\\nGraphPad Prism\\nIBM SPSS Statistics\\nIBM SPSS Modeler\\nJMP\\nMaple\\nMathcad\\nMathematica\\nMATLAB\\nOxMetrics\\nRATS\\nRevolution Analytics\\nSAS\\nSmartPLS\\nStata\\nStatView\\nSUDAAN\\nS-PLUS\\nTSP\\nWorld Programming System (WPS)\\nWindows only\\nBMDP\\nEViews\\nGenStat\\nLIMDEP\\nLISREL\\nMedCalc\\nMicrofit\\nMinitab\\nMLwiN\\nNCSS\\nSHAZAM\\nSigmaStat\\nStatistica\\nStatsDirect\\nStatXact\\nSYSTAT\\nThe Unscrambler\\nUNISTAT\\nExcel add-ons\\nAnalyse-it\\nUNISTAT for Excel\\nXLfit\\nRExcel\\n\\nCategory\\nComparison\\n\\nvteNumerical-analysis softwareFree\\nAdvanced Simulation Library\\nADMB\\nChapel\\nEuler\\nFreeFem++\\nFreeMat\\nGenius\\nGmsh\\nGNU Octave\\ngretl\\nJulia\\nJupyter (Julia, Python, R; IPython)\\nMFEM\\nOpenFOAM\\nPython\\nR\\nSageMath\\nSalome\\nScicosLab\\nScilab\\nX10\\nWeka\\nDiscontinued\\nFortress\\n\\nProprietary\\nDADiSP\\nFEATool Multiphysics\\nGAUSS\\nLabVIEW\\nMaple\\nMathcad\\nMathematica\\nMATLAB\\nSpeakeasy\\nVisSim\\n\\nComparison\\n\\nAuthority control databases InternationalFASTNationalGermanyUnited StatesFranceBnF dataCzech RepublicIsraelOtherIdRef\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Python_(programming_language)&oldid=1251970759\"\\nCategories: Python (programming language)Class-based programming languagesNotebook interfaceComputer science in the NetherlandsConcurrent programming languagesCross-platform free softwareCross-platform softwareDutch inventionsDynamically typed programming languagesEducational programming languagesHigh-level programming languagesInformation technology in the NetherlandsMulti-paradigm programming languagesObject-oriented programming languagesPattern matching programming languagesProgramming languagesProgramming languages created in 1991Scripting languagesText-oriented programming languagesHidden categories: Articles with short descriptionShort description matches WikidataUse dmy dates from November 2021Articles containing potentially dated statements from October 2024All articles containing potentially dated statementsArticles containing potentially dated statements from March 2024Articles containing potentially dated statements from December 2022Articles containing potentially dated statements from 2020Articles containing potentially dated statements from 2008Pages using Sister project links with wikidata namespace mismatchPages using Sister project links with hidden wikidataArticles with example Python (programming language) code\\n\\n\\n\\n\\n\\n\\n This page was last edited on 19 October 2024, at 02:59\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data ingestion with web based url\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Python_(programming_language)\")\n",
    "text_docs = loader.load()\n",
    "text_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data ingestion with pdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "pdf = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i)=sin(pos/100002i/d model)\\nP E(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,P Epos+kcan be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6}, page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Text: It breaks down long pieces of text into smaller chunks. In this case, each chunk will have a maximum size of 1000 characters.\n",
    "### Overlap: It ensures that the parts overlap by 200 characters to avoid losing important information.\n",
    "### Recursive Splitting: It tries to split by larger divisions like paragraphs first, then smaller ones like sentences if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 1}, page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 2}, page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 2}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by√dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 3}, page_content='into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n√dk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1√dk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 3}, page_content='dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 4}, page_content='The Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n•The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 4}, page_content='encoder.\\n•Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 4}, page_content='FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by√dmodel.\\n5'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i)=sin(pos/100002i/d model)\\nP E(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,P Epos+kcan be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 5}, page_content='executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6}, page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6}, page_content='orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6}, page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6}, page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with β1= 0.9,β2= 0.98andϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d−0.5\\nmodel·min(step_num−0.5, step _num·warmup _steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0·1020\\nGNMT + RL [38] 24.6 39.92 2.3·10191.4·1020\\nConvS2S [9] 25.16 40.46 9.6·10181.5·1020\\nMoE [32] 26.03 40.56 2.0·10191.2·1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0·1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8·10201.1·1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7·10191.2·1021\\nTransformer (base model) 27.3 38.1 3.3·1018\\nTransformer (big) 28.4 41.8 2.3·1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 7}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 7}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty α= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 7}, page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop ϵlstrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 8}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 8}, page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 8}, page_content='Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andα= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 9}, page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 9}, page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 9}, page_content='comments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 11}, page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 11}, page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 11}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## split the data into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_pdf = text_splitter.split_documents(pdf)\n",
    "text_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 0}, page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pdf[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/llm/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "## Vector Embedding and Vector Store\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "db = Chroma.from_documents(text_pdf, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 2, 'source': 'attention.pdf'}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'page': 6, 'source': 'attention.pdf'}, page_content='orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'page': 12, 'source': 'attention.pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'page': 10, 'source': 'attention.pdf'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query and get the result\n",
    "query = \"Who are the authors of attention is all you need?\"\n",
    "docs = db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x162669c90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get and store with another vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "faiss_db = FAISS.from_documents(text_pdf, embeddings)\n",
    "faiss_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention.pdf', 'page': 2}, page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 6}, page_content='orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'source': 'attention.pdf', 'page': 10}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query and get the result\n",
    "query = \"Who are the authors of attention is all you need?\"\n",
    "docs = faiss_db.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
