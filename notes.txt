How to use LLM models for chatbot
1. Install ollama
2. Download the model which u want
        Model	Parameters	Size	Download
        Llama 3.2	3B	2.0GB	ollama run llama3.2
        Llama 3.2	1B	1.3GB	ollama run llama3.2:1b
        Llama 3.1	8B	4.7GB	ollama run llama3.1
        Llama 3.1	70B	40GB	ollama run llama3.1:70b
        Llama 3.1	405B	231GB	ollama run llama3.1:405b
        Phi 3 Mini	3.8B	2.3GB	ollama run phi3
        Phi 3 Medium	14B	7.9GB	ollama run phi3:medium
        Gemma 2	2B	1.6GB	ollama run gemma2:2b
        Gemma 2	9B	5.5GB	ollama run gemma2
        Gemma 2	27B	16GB	ollama run gemma2:27b
        Mistral	7B	4.1GB	ollama run mistral
        Moondream 2	1.4B	829MB	ollama run moondream
        Neural Chat	7B	4.1GB	ollama run neural-chat
        Starling	7B	4.1GB	ollama run starling-lm
        Code Llama	7B	3.8GB	ollama run codellama
        Llama 2 Uncensored	7B	3.8GB	ollama run llama2-uncensored
        LLaVA	7B	4.5GB	ollama run llava
        Solar	10.7B	6.1GB	ollama run solar
3. pull the model on your project follow the commands
ollama pull llama3.2

IMP: Always remember that to use llama3.2 or any other model it should run backend
to run this use cmd: ollama run llama3.2

===================================================================================
Rag Pipeline:

A RAG (Retrieval-Augmented Generation) pipeline is a way for AI to answer questions by using two steps:

Find information (Retrieval): When you ask a question, the system first looks for relevant information 
from a database or documents. For example, if you ask about the weather, it will find recent weather reports.

Create a response (Generation): After finding the information, the system uses an AI model (like ChatGPT) 
to create a clear and useful answer based on both the information it found and your question.

load data -> convert data into chunks -> apply embedding into chunks -> store into vecots data base -> Query and Retrieve Relevant Chunks
-> Combine and Generate a Response -> Return the Answer

====================================================================================
Chain and Retrieveer
A chain in LangChain is a sequence of actions or processes that take input, transform it, and produce an output.
A retriever is a component that helps in fetching relevant data from a database or document source based on a query.

user input -> retriever -> LLM,Prompt [create_stuff_documents_chain] after that we wil get the response
1. user input 
2. that input will go to the DB vector srores
3. Then it will go tht LLM and prompt 
4. then we got the answer

the process in code:
1. load the data and breake into chunks
2. convert that chunks into vectors and store in Vector DB
3. Initialize the model and prompts
for ollama please run the model in backend
4. combine the both model and prompts using below function which provided by langchain
from langchain.chains.combine_documents import create_stuff_documents_chain
documents_chain = create_stuff_documents_chain(llm, prompt)

5. Make the DB as retriever
DB.as_retriever()

6. combine the retriever with the documents chain
from langchain.chains import create_retrieval_chain
ret_chain = create_retrieval_chain(db,documents_chain)

7. give the input and take the output
response = ret_chain.invoke({"input":"Scaled Dot-Product Attention"})
response['answer']

=================================================================================================
Multiple Nodes Search 
like wiki+arxiv+other Search[ex: pdf] -> llm ->

agents in LangChain are like smart helpers that can figure out what steps to take to complete a task. Here’s what they do:

Make Decisions: They think about the problem and decide what to do next. For example, if you ask a question, 
the agent will figure out if it needs to look something up, do a calculation, or ask for more information.

Use Tools: Agents can use different tools like a search engine or a database to find the right information. 
They pick the right tool based on the task.

Adaptable: They don’t follow a strict set of rules. Instead, they adapt and choose actions step-by-step based on what they 
learn along the way.

code:
api_wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)
wiki = WikipediaQueryRun(api_wrapper=api_wrapper)

WikipediaAPIWrapper:
This class allows you to search Wikipedia.
It limits the number of results (in this case, only 1 result) and controls how much of the article's text you get(up to 200 characters).

WikipediaQueryRun:
This class runs the actual Wikipedia search using the WikipediaAPIWrapper.
When you ask it a question, it fetches relevant information from Wikipedia based on the limits you set.

create_retriever_tool is more about creating a specialized tool for complex workflows, while db.as_retriever() is a simple way to 
get a retriever from your vector database for immediate use.

in simple english "create_retriever_tool" is used to make "db.as_retriever()" as a tool for other pipelines in langchain

====================================================================
session_state in streamlit:

st.session_state is like a notebook for your Streamlit app. It lets you save information (like user input or selections) 
so it doesn’t get lost when the app reloads.

Example:
Imagine you have a form where a user types their name. Every time they hit a button, the app refreshes, and the name would 
usually disappear.
With st.session_state, the app "remembers" the name even after the user interacts with other parts of the app.