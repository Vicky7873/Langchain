How to use LLM models for chatbot
1. Install ollama
2. Download the model which u want
        Model	Parameters	Size	Download
        Llama 3.2	3B	2.0GB	ollama run llama3.2
        Llama 3.2	1B	1.3GB	ollama run llama3.2:1b
        Llama 3.1	8B	4.7GB	ollama run llama3.1
        Llama 3.1	70B	40GB	ollama run llama3.1:70b
        Llama 3.1	405B	231GB	ollama run llama3.1:405b
        Phi 3 Mini	3.8B	2.3GB	ollama run phi3
        Phi 3 Medium	14B	7.9GB	ollama run phi3:medium
        Gemma 2	2B	1.6GB	ollama run gemma2:2b
        Gemma 2	9B	5.5GB	ollama run gemma2
        Gemma 2	27B	16GB	ollama run gemma2:27b
        Mistral	7B	4.1GB	ollama run mistral
        Moondream 2	1.4B	829MB	ollama run moondream
        Neural Chat	7B	4.1GB	ollama run neural-chat
        Starling	7B	4.1GB	ollama run starling-lm
        Code Llama	7B	3.8GB	ollama run codellama
        Llama 2 Uncensored	7B	3.8GB	ollama run llama2-uncensored
        LLaVA	7B	4.5GB	ollama run llava
        Solar	10.7B	6.1GB	ollama run solar
3. pull the model on your project follow the commands
ollama pull llama3.2

===================================================================================
Rag Pipeline:

A RAG (Retrieval-Augmented Generation) pipeline is a way for AI to answer questions by using two steps:

Find information (Retrieval): When you ask a question, the system first looks for relevant information 
from a database or documents. For example, if you ask about the weather, it will find recent weather reports.

Create a response (Generation): After finding the information, the system uses an AI model (like ChatGPT) 
to create a clear and useful answer based on both the information it found and your question.

load data -> convert data into chunks -> apply embedding into chunks -> store into vecots data base -> Query and Retrieve Relevant Chunks
-> Combine and Generate a Response -> Return the Answer 